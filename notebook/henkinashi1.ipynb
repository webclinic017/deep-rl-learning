{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T17:21:35.959921Z",
     "start_time": "2022-04-16T17:21:35.938933Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from plotly.subplots import make_subplots\n",
    "import talib\n",
    "from talib import CDLENGULFING\n",
    "import colored\n",
    "import os\n",
    "import yfinance\n",
    "import time\n",
    "from mpl_finance import candlestick_ohlc\n",
    "import matplotlib.dates as mpl_dates\n",
    "import matplotlib.pyplot as plt\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images\")\n",
    "from talib import EMA, MACD\n",
    "from termcolor import colored\n",
    "import chart_studio.plotly as py\n",
    "from IPython.display import clear_output\n",
    "import pyautogui\n",
    "from scipy.stats import linregress\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import math\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import argrelextrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T17:21:36.838108Z",
     "start_time": "2022-04-16T17:21:36.816110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytz\n",
    "import MetaTrader5 as mt5\n",
    "mt5.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T15:02:13.106696Z",
     "start_time": "2022-04-16T15:02:13.076697Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_text(trend):\n",
    "    if trend == 'Buy':\n",
    "        return colored(trend, 'green')\n",
    "    if trend == 'Sell':\n",
    "        return colored(trend, 'red')\n",
    "    return colored('Neutral', 'blue')\n",
    "\n",
    "def heikin_ashi(df):\n",
    "    heikin_ashi_df = pd.DataFrame(index=df.index.values, columns=['open', 'high', 'low', 'close'])\n",
    "    \n",
    "    heikin_ashi_df['close'] = (df['open'] + df['high'] + df['low'] + df['close']) / 4\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if i == 0:\n",
    "            heikin_ashi_df.iat[0, 0] = df['open'].iloc[0]\n",
    "        else:\n",
    "            heikin_ashi_df.iat[i, 0] = (heikin_ashi_df.iat[i-1, 0] + heikin_ashi_df.iat[i-1, 3]) / 2\n",
    "        \n",
    "    heikin_ashi_df['high'] = heikin_ashi_df.loc[:, ['open', 'close']].join(df['high']).max(axis=1)\n",
    "    \n",
    "    heikin_ashi_df['low'] = heikin_ashi_df.loc[:, ['open', 'close']].join(df['low']).min(axis=1)\n",
    "    heikin_ashi_df['Date'] = df['Date']\n",
    "    return heikin_ashi_df\n",
    "\n",
    "def format_text(trend):\n",
    "    if trend == 'Buy':\n",
    "        return colored(trend, 'green')\n",
    "    if trend == 'Sell':\n",
    "        return colored(trend, 'red')\n",
    "    if trend == 'Close_Buy':\n",
    "        return colored(trend, 'yellow')\n",
    "    if trend == 'Close_Sell':\n",
    "        return colored(trend, 'yellow')\n",
    "    if trend == 'Neutral':\n",
    "        return colored(trend, 'cyan')\n",
    "    return colored('0', 'blue')\n",
    "def calculate_slope(signal):\n",
    "    x = np.array([x for x in range(len(signal.index))])\n",
    "    scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "    y = scaler.fit_transform(signal.values.reshape(-1, 1))\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y.flatten())\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T15:02:13.138695Z",
     "start_time": "2022-04-16T15:02:13.109697Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_trend(close_p: float, sp_trend_1: float) -> str:\n",
    "    if close_p > sp_trend_1:\n",
    "        return \"Buy\"\n",
    "    if close_p < sp_trend_1:\n",
    "        return \"Sell\"\n",
    "    return \"No Trend\"\n",
    "\n",
    "def isSupport(df,i):\n",
    "    support = df['Low'][i] < df['Low'][i-1] and \\\n",
    "        df['Low'][i] < df['Low'][i+1] and \\\n",
    "        df['Low'][i+1] < df['Low'][i+2] and \\\n",
    "        df['Low'][i-1] < df['Low'][i-2]\n",
    "\n",
    "    return support\n",
    "\n",
    "\n",
    "def isResistance(df,i):\n",
    "    resistance = df['High'][i] > df['High'][i-1]  and \\\n",
    "        df['High'][i] > df['High'][i+1] and \\\n",
    "        df['High'][i+1] > df['High'][i+2] and \\\n",
    "        df['High'][i-1] > df['High'][i-2] \n",
    "\n",
    "    return resistance\n",
    "\n",
    "\n",
    "def isFarFromLevel(l, s, levels):\n",
    "    return np.sum([abs(l-x[1]) < s  for x in levels]) == 0\n",
    "\n",
    "\n",
    "def calculate_support_registance(start, stop):\n",
    "    levels = []\n",
    "    df_new = df.iloc[start:stop].reset_index(drop=True)\n",
    "    s =  np.mean(df_new['High'] - df_new['Low'])\n",
    "    for i in range(2,df_new.shape[0]-2):\n",
    "        if isSupport(df_new,i):\n",
    "            l = df_new['Low'][i]\n",
    "            if isFarFromLevel(l, s, levels):\n",
    "                levels.append((i,l, 'Support'))\n",
    "\n",
    "        elif isResistance(df_new,i):\n",
    "            l = df_new['High'][i]\n",
    "            if isFarFromLevel(l, s, levels):\n",
    "                levels.append((i,l, 'Resistance'))\n",
    "    return levels\n",
    "\n",
    "def save_result(order_index, df_index, trend):\n",
    "    return True\n",
    "    new_df = df.iloc[order_index:df_index]\n",
    "    data=[\n",
    "        go.Candlestick(x=new_df.index, open=new_df['open'], high=new_df['high'], low=new_df['low'], close=new_df['close']),\n",
    "    ]\n",
    "    fig = go.Figure(data)\n",
    "    fig.add_hline(y=df.close[order_index])\n",
    "#     fig.add_vline(x=df.index[order_index], line_width=3, line_dash=\"dash\", line_color=\"green\")\n",
    "    fig.update_layout(xaxis_rangeslider_visible=False)\n",
    "    fig.write_image(f\"images/{order_index}-{trend}.png\")\n",
    "    \n",
    "def finonaci(df_check):\n",
    "    price_min = df_check.Close.min()\n",
    "    price_max = df_check.Close.max()\n",
    "    diff = abs(price_max - price_min)\n",
    "    level1 = 0.236 * diff\n",
    "    level2 = price_max - 0.382 * diff\n",
    "    level3 = price_max - 0.618 * diff\n",
    "#     print(price_min, price_max, diff, level1)\n",
    "    return level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T15:02:13.404896Z",
     "start_time": "2022-04-16T15:02:13.140697Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_valid_uploaded_filename(filename, folder):\n",
    "        \"\"\"\n",
    "        Check whether uploaded file already exists. If yes, rename it\n",
    "\n",
    "        :param filename: uploaded file\n",
    "        :type filename: str\n",
    "        :param folder: folder into save the file\n",
    "        :type folder: str\n",
    "        :return: unique filename\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        file_query_s = os.path.join(folder, filename)\n",
    "        i = 2\n",
    "        filename_orig = filename\n",
    "        while os.path.exists(file_query_s):\n",
    "            filename = str(i) + \"_\" + filename_orig\n",
    "            file_query_s = os.path.join(folder, filename)\n",
    "            i += 1\n",
    "        return filename \n",
    "    \n",
    "def plot_chart(df, pivots, order_label, order_date, stop_loss, signal_slope, x_signal, minima, maxima):\n",
    "    # Set colours for up and down candles\n",
    "    INCREASING_COLOR = '#26a69a'\n",
    "    DECREASING_COLOR = '#ef5350'\n",
    "    \n",
    "    # create list to hold dictionary with data for our first series to plot\n",
    "    # (which is the candlestick element itself)\n",
    "    data = [ \n",
    "        dict(\n",
    "            type = 'candlestick',\n",
    "            open = df.open,\n",
    "            high = df.high,\n",
    "            low = df.low,\n",
    "            close = df.close,\n",
    "            x = df.Date,\n",
    "            yaxis = 'y3',\n",
    "            name = 'candle stick chart',\n",
    "            increasing = dict( line = dict( color = INCREASING_COLOR ) ),\n",
    "            decreasing = dict( line = dict( color = DECREASING_COLOR ) ),\n",
    "        ),dict(\n",
    "            type = 'scatter',\n",
    "            name = \"SIGNAL\", \n",
    "            mode = \"lines\",\n",
    "            x = df.Date,\n",
    "            y = df.SIGNAL,\n",
    "            yaxis = 'y2'\n",
    "        ),dict(\n",
    "            type = 'scatter',\n",
    "            name = \"MACD\", \n",
    "            x = df.Date,\n",
    "            y = df.MACD,\n",
    "            yaxis = 'y2'\n",
    "        ),dict(\n",
    "            type = 'bar',\n",
    "            name = \"MACD Histogram\", \n",
    "            x = df.Date,\n",
    "            y = df.HIST,\n",
    "            yaxis = 'y2'\n",
    "        ),dict(\n",
    "            type = 'scatter',\n",
    "            mode = \"lines\", \n",
    "            name = \"RSI\", \n",
    "            x = df.Date,\n",
    "            y = df.RSI\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Create empty dictionary for later use to hold settings and layout options\n",
    "    layout = {\n",
    "      \"yaxis\": {\"domain\": [0, 0.33], \"visible\": False}, \n",
    "      \"yaxis2\": {\"domain\": [0.33, 0.66], \"visible\": False}, \n",
    "      \"yaxis3\": {\"domain\": [0.66, 1], \"visible\": False},\n",
    "      \"xaxis\": {\"rangeslider\": {\"visible\": False}, \"visible\": False},\n",
    "      'showlegend': False\n",
    "    }\n",
    "\n",
    "    # create our main chart \"Figure\" object which consists of data to plot and layout settings\n",
    "    fig = dict(data=data, layout=layout)\n",
    "\n",
    "    # Assign various seeting and choices - background colour, range selector etc\n",
    "    fig['layout']['plot_bgcolor'] = 'rgb(250, 250, 250)'\n",
    "    #fig['layout']['xaxis'] = dict( rangeselector = dict( visible = False ) )\n",
    "#     fig['layout']['yaxis'] = dict( domain = [0, 0.2], showticklabels = False )\n",
    "#     fig['layout']['yaxis2'] = dict( domain = [0.2, 0.8] )\n",
    "    #fig['layout']['legend'] = dict( orientation = 'h', y=0.9, x=0.3, yanchor='bottom' )\n",
    "    fig['layout']['margin'] = dict( t=40, b=40, r=40, l=40 )\n",
    "    fig['layout']['width'] = 1920\n",
    "    fig['layout']['height'] = 1080\n",
    "    \n",
    "    # Populate the \"rangeselector\" object with necessary settings\n",
    "    rangeselector=dict(\n",
    "        visible = False,\n",
    "        x = 0, y = 0.9,\n",
    "        bgcolor = 'rgba(150, 200, 250, 0.4)',\n",
    "        font = dict( size = 13 ),\n",
    "        buttons=list([\n",
    "            dict(count=1,\n",
    "                 label='reset',\n",
    "                 step='all'),\n",
    "            dict(count=1,\n",
    "                 label='1yr',\n",
    "                 step='year',\n",
    "                 stepmode='backward'),\n",
    "            dict(count=3,\n",
    "                label='3 mo',\n",
    "                step='month',\n",
    "                stepmode='backward'),\n",
    "            dict(count=1,\n",
    "                label='1 mo',\n",
    "                step='month',\n",
    "                stepmode='backward'),\n",
    "            dict(step='all')\n",
    "        ]))\n",
    "\n",
    "\n",
    "    # fig['layout']['xaxis']['rangeselector'] = rangeselector\n",
    "\n",
    "    # Append the Ichimoku elements to the plot\n",
    "    fig['data'].append( dict( x=df.Date, y=df['tenkan_sen'], type='scatter', mode='lines', \n",
    "                             line = dict( width = 2 ),\n",
    "                             marker = dict( color = '#33BDFF' ),\n",
    "                             yaxis = 'y3', name='tenkan_sen' ) )\n",
    "\n",
    "    fig['data'].append( dict( x=df.Date, y=df['kijun_sen'], type='scatter', mode='lines', \n",
    "                             line = dict( width = 2 ),\n",
    "                             marker = dict( color = '#F1F316' ),\n",
    "                             yaxis = 'y3', name='kijun_sen' ) )\n",
    "\n",
    "    fig['data'].append( dict( x=df.Date, y=df['senkou_span_a'], type='scatter', mode='lines', \n",
    "                             line = dict( width = 1 ), \n",
    "                             marker = dict( color = '#228B22' ),\n",
    "                             yaxis = 'y3', name='senkou_span_a' ) )\n",
    "\n",
    "    fig['data'].append( dict( x=df.Date, y=df['senkou_span_b'], type='scatter', mode='lines', \n",
    "                             line = dict( width = 1 ),fill='tonexty',\n",
    "                             marker = dict( color = '#FF3342' ),\n",
    "                             yaxis = 'y3', name='senkou_span_b' ) )\n",
    "\n",
    "    fig['data'].append( dict( x=df.Date, y=df['chikou_span'], type='scatter', mode='lines', \n",
    "                             line = dict( width = 1 ),\n",
    "                             marker = dict( color = '#D105F5' ),\n",
    "                             yaxis = 'y3', name='chikou_span' ) )\n",
    "    have_resistance = False\n",
    "    have_support = False\n",
    "    for pivot in reversed(pivots):\n",
    "        pivot_idx, pivot_price, pivot_type = pivot\n",
    "        if pivot_type == \"resistance\":\n",
    "            have_resistance = True\n",
    "        if pivot_type == \"support\":\n",
    "            have_support = True\n",
    "\n",
    "        y_sample = [pivot_price if pivot_idx < index else None for index in range(len(df.index))]\n",
    "        x_sample = [date if pivot_idx < index else None for index, date in enumerate(df['Date'])]\n",
    "\n",
    "        fig['data'].append( dict( x=x_sample, y=y_sample, type='scatter', mode='lines', \n",
    "                             line = dict( width = 3 ),\n",
    "                             marker = dict( color = '#eb346e' if pivot_type == \"resistance\" else \"#52eb34\" ),\n",
    "                             yaxis = 'y3', name='pivots' ) )\n",
    "        if have_support and have_resistance:\n",
    "            break\n",
    "    if stop_loss:\n",
    "        y_sample = [stop_loss]*len(df.index)\n",
    "        fig['data'].append( dict( x=df['Date'], y=y_sample, type='scatter', mode='lines', \n",
    "                     line = dict( width = 3 ),\n",
    "                     marker = dict( color = '#eb346e' if pivot_type == \"resistance\" else \"#52eb34\" ),\n",
    "                     yaxis = 'y3', name='pivots' ) )\n",
    "\n",
    "    # Set colour list for candlesticks\n",
    "    colors = []\n",
    "\n",
    "    for i in range(len(df.close)):\n",
    "        if i != 0:\n",
    "            if df.close[i] > df.close[i-1]:\n",
    "                colors.append(INCREASING_COLOR)\n",
    "            else:\n",
    "                colors.append(DECREASING_COLOR)\n",
    "        else:\n",
    "            colors.append(DECREASING_COLOR)\n",
    "    for index, date in enumerate(df['Date']):\n",
    "        if date == order_date:\n",
    "            # print(order_date, df.close.iat[index])\n",
    "            fig['data'].append(dict(x=[order_date], y=[df.close.iat[index]], yaxis = 'y3', marker=dict(color='#3256a8')))\n",
    "            break\n",
    "    \n",
    "    for item in minima:\n",
    "        fig['data'].append(dict(x=[df['Date'].iat[item]], y=[df.SIGNAL.iat[item]], yaxis = 'y2', marker=dict(color='#42f554')))\n",
    "\n",
    "    for item in maxima:\n",
    "        fig['data'].append(dict(x=[df['Date'].iat[item]], y=[df.SIGNAL.iat[item]], yaxis = 'y2', marker=dict(color='#f54254')))\n",
    "\n",
    "    figg = go.Figure(fig)\n",
    "    order_date = order_date.replace(\":\", \"\")\n",
    "    if order_label >= 0:\n",
    "        file_name = f\"images/1/{order_date} {order_label} {signal_slope}.png\"\n",
    "    else:\n",
    "        file_name = f\"images/0/{order_date} {order_label} {signal_slope}.png\"\n",
    "    figg.write_image(file_name)\n",
    "    # py.iplot(fig, filename='Sine Wave Slider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T15:02:13.451894Z",
     "start_time": "2022-04-16T15:02:13.406887Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#method 2: window shifting method\n",
    "#using the same symbol as the first example above\n",
    "\n",
    "# for visualization\n",
    "def plot_all(levels, df):    \n",
    "#     fig, ax = plt.subplots(figsize=(16, 9))   \n",
    "#     date_format = mpl_dates.DateFormatter('%Y-%b-%d')\n",
    "    f1 = plt.subplot2grid((6, 4), (1, 0), rowspan=6, colspan=4)\n",
    "    # f1.xaxis.set_major_formatter(mdates.DateFormatter('%y-%m-%d'))\n",
    "    candlestick_ohlc(f1, df.values, width=.6, colorup='#53c156', colordown='#ff1717')\n",
    "    for level in levels:        \n",
    "        plt.hlines(level[1], xmin = df['index'][level[0]], xmax = max(df['index']), colors='blue', linestyle='--')    \n",
    "    plt.show()\n",
    "\n",
    "def is_support(df,i):  \n",
    "    cond1 = df['low'][i] < df['low'][i-1]   \n",
    "    cond2 = df['low'][i] < df['low'][i+1]   \n",
    "    cond3 = df['low'][i+1] < df['low'][i+2]   \n",
    "    cond4 = df['low'][i-1] < df['low'][i-2]  \n",
    "    return (cond1 and cond2 and cond3 and cond4) \n",
    "# determine bearish fractal\n",
    "def is_resistance(df,i):  \n",
    "    cond1 = df['high'][i] > df['high'][i-1]   \n",
    "    cond2 = df['high'][i] > df['high'][i+1]   \n",
    "    cond3 = df['high'][i+1] > df['high'][i+2]   \n",
    "    cond4 = df['high'][i-1] > df['high'][i-2]  \n",
    "    return (cond1 and cond2 and cond3 and cond4)\n",
    "\n",
    "# to make sure the new level area does not exist already\n",
    "def is_far_from_level(value, levels, df):    \n",
    "    ave =  np.mean(df['high'] - df['low'])    \n",
    "    return np.sum([abs(value-level)<ave for _,level,_ in levels])==0\n",
    "\n",
    "def get_pivot(df):\n",
    "    pivots = []\n",
    "    max_list = []\n",
    "    min_list = []\n",
    "    for i in range(5, len(df)-5):\n",
    "        # taking a window of 9 candles\n",
    "        high_range = df['close'][i-5:i+4]\n",
    "        current_max = high_range.max()\n",
    "        # if we find a new maximum value, empty the max_list \n",
    "        if current_max not in max_list:\n",
    "            max_list = []\n",
    "        max_list.append(current_max)\n",
    "        # if the maximum value remains the same after shifting 5 times\n",
    "        if len(max_list)==5 and is_far_from_level(current_max,pivots,df):\n",
    "            pivots.append((high_range.idxmax(), current_max, \"resistance\"))\n",
    "\n",
    "        low_range = df['close'][i-5:i+5]\n",
    "        current_min = low_range.min()\n",
    "        if current_min not in min_list:\n",
    "            min_list = []\n",
    "        min_list.append(current_min)\n",
    "        if len(min_list)==5 and is_far_from_level(current_min,pivots,df):\n",
    "            pivots.append((low_range.idxmin(), current_min, \"support\"))\n",
    "    return pivots\n",
    "\n",
    "\n",
    "# test_ragnge = 1000\n",
    "# for idx in range(test_ragnge):\n",
    "# mt5.copy_rates_from()\n",
    "# rates_frame = mt5.copy_rates_from('BTCUSDm', mt5.TIMEFRAME_H1, datetime.now(), 150)\n",
    "# df = pd.DataFrame(rates_frame, columns=['time', 'open', 'high', 'low', 'close'])\n",
    "# df['Date'] = pd.to_datetime(df['time'], unit='s')\n",
    "# df = df.drop(['time'], axis=1)\n",
    "# df['Date'] = df.Date.dt.strftime('%Y.%m.%d %H:%M:%S')\n",
    "# df = df.reindex(columns=['Date', 'open', 'high', 'low', 'close'])\n",
    "# # convert to float to avoid sai so.\n",
    "# df.open = df.open.astype(float)\n",
    "# df.high = df.high.astype(float)\n",
    "# df.low = df.low.astype(float)\n",
    "# df.close = df.close.astype(float)\n",
    "# df['RSI'] = talib.RSI(df.close)\n",
    "# exp1 = df.close.ewm(span=12, adjust=False).mean()\n",
    "# exp2 = df.close.ewm(span=26, adjust=False).mean()\n",
    "# df['MACD'] = exp1 - exp2\n",
    "# df['SIGNAL'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "# df['HIST'] = df['MACD'] - df['SIGNAL']\n",
    "# df['HIST_NORM'] = df.HIST.ewm(span=20, adjust=False).mean()\n",
    "# # Tenkan-sen (Conversion Line): (9-period high + 9-period low)/2))\n",
    "# nine_period_high = df['high'].rolling(window= 9).max()\n",
    "# nine_period_low = df['low'].rolling(window= 9).min()\n",
    "# df['tenkan_sen'] = (nine_period_high + nine_period_low) /2\n",
    "# # Kijun-sen (Base Line): (26-period high + 26-period low)/2))\n",
    "# period26_high = df['high'].rolling(window=26).max()\n",
    "# period26_low = df['low'].rolling(window=26).min()\n",
    "# df['kijun_sen'] = (period26_high + period26_low) / 2\n",
    "# # Senkou Span A (Leading Span A): (Conversion Line + Base Line)/2))\n",
    "# df['senkou_span_a'] = ((df['tenkan_sen'] + df['kijun_sen']) / 2).shift(26)\n",
    "# # Senkou Span B (Leading Span B): (52-period high + 52-period low)/2))\n",
    "# period52_high = df['high'].rolling(window=52).max()\n",
    "# period52_low = df['low'].rolling(window=52).min()\n",
    "# df['senkou_span_b'] = ((period52_high + period52_low) / 2).shift(26)\n",
    "# # The most current closing price plotted 26 time periods behind (optional)\n",
    "# df['chikou_span'] = df['close'].shift(-26)\n",
    "# # Tenkan-sen (Conversion Line): (9-period high + 9-period low)/2))\n",
    "# macd, macdsignal, macdhist = talib.MACD(df.close)\n",
    "# macd = macdsignal.dropna()[-15:]\n",
    "# pivots = get_pivot(df)\n",
    "# plot_chart(df, pivots, \"\", \"\", None)\n",
    "#time.sleep(0.25)\n",
    "#clear_output(wait=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T12:19:27.823692Z",
     "start_time": "2022-04-20T12:19:27.780691Z"
    }
   },
   "outputs": [],
   "source": [
    "def ichimoku_cloud(utc_to, timeframe, symbol, save_frame, order_label, order_date, stop_loss):\n",
    "    rates_frame = mt5.copy_rates_from(symbol, timeframe, utc_to + timedelta(hours=2), 100)\n",
    "    rates_frame = rates_frame.copy()\n",
    "    # create DataFrame out of the obtained data\n",
    "    \n",
    "    if timeframe == mt5.TIMEFRAME_W1:\n",
    "        num_frame = utc_to.weekday() + 1\n",
    "        rates_frame_fixed = mt5.copy_rates_from(symbol, mt5.TIMEFRAME_D1, utc_to + timedelta(hours=2), num_frame)\n",
    "        df_fixed = pd.DataFrame(rates_frame_fixed, columns=['time', 'open', 'high', 'low', 'close'])\n",
    "        df_fixed['Date'] = pd.to_datetime(df_fixed['time'], unit='s')\n",
    "        df_fixed['Date'] = df_fixed.Date.dt.strftime('%Y.%m.%d %H:%M:%S')\n",
    "        df_fixed = df_fixed.drop(['time'], axis=1)\n",
    "        df_fixed = df_fixed.reindex(columns=['Date', 'open', 'high', 'low', 'close'])\n",
    "\n",
    "        new_open, new_high, new_low, new_close = df_fixed.open.iat[0], np.amax(df_fixed.high), np.amin(df_fixed.low), df_fixed.close.iat[-1]\n",
    "        current_date = pd.to_datetime(rates_frame[-1][0], unit='s').strftime('%Y.%m.%d %H:%M:%S')\n",
    "\n",
    "        rates_frame[-1][0] = rates_frame_fixed[-1][0]\n",
    "        rates_frame[-1][1] = new_open\n",
    "        rates_frame[-1][2] = new_high\n",
    "        rates_frame[-1][3] = new_low\n",
    "        rates_frame[-1][4] = new_close\n",
    "\n",
    "    if timeframe == mt5.TIMEFRAME_D1:\n",
    "        num_frame = utc_to.hour % 24 + 1\n",
    "        rates_frame_fixed = mt5.copy_rates_from(symbol, mt5.TIMEFRAME_H1, utc_to + timedelta(hours=2), num_frame)\n",
    "        df_fixed = pd.DataFrame(rates_frame_fixed, columns=['time', 'open', 'high', 'low', 'close'])\n",
    "        df_fixed['Date'] = pd.to_datetime(df_fixed['time'], unit='s')\n",
    "        df_fixed['Date'] = df_fixed.Date.dt.strftime('%Y.%m.%d %H:%M:%S')\n",
    "        df_fixed = df_fixed.drop(['time'], axis=1)\n",
    "        df_fixed = df_fixed.reindex(columns=['Date', 'open', 'high', 'low', 'close'])\n",
    " \n",
    "        new_open, new_high, new_low, new_close = df_fixed.open.iat[0], np.amax(df_fixed.high), np.amin(df_fixed.low), df_fixed.close.iat[-1]\n",
    "        current_date = pd.to_datetime(rates_frame[-1][0], unit='s').strftime('%Y.%m.%d %H:%M:%S')\n",
    "\n",
    "        rates_frame[-1][0] = rates_frame_fixed[-1][0]\n",
    "        rates_frame[-1][1] = new_open\n",
    "        rates_frame[-1][2] = new_high\n",
    "        rates_frame[-1][3] = new_low\n",
    "        rates_frame[-1][4] = new_close\n",
    "\n",
    "    if timeframe == mt5.TIMEFRAME_H12:\n",
    "        num_frame = utc_to.hour % 12 + 1\n",
    "        rates_frame_fixed = mt5.copy_rates_from(symbol, mt5.TIMEFRAME_H1, utc_to + timedelta(hours=2), num_frame)\n",
    "        df_fixed = pd.DataFrame(rates_frame_fixed, columns=['time', 'open', 'high', 'low', 'close'])\n",
    "        df_fixed['Date'] = pd.to_datetime(df_fixed['time'], unit='s')\n",
    "        df_fixed['Date'] = df_fixed.Date.dt.strftime('%Y.%m.%d %H:%M:%S')\n",
    "        df_fixed = df_fixed.drop(['time'], axis=1)\n",
    "        df_fixed = df_fixed.reindex(columns=['Date', 'open', 'high', 'low', 'close'])\n",
    "        \n",
    "        new_open, new_high, new_low, new_close = df_fixed.open.iat[0], np.amax(df_fixed.high), np.amin(df_fixed.low), df_fixed.close.iat[-1]\n",
    "        current_date = pd.to_datetime(rates_frame[-1][0], unit='s').strftime('%Y.%m.%d %H:%M:%S')\n",
    "\n",
    "        rates_frame[-1][0] = rates_frame_fixed[-1][0]\n",
    "        rates_frame[-1][1] = new_open\n",
    "        rates_frame[-1][2] = new_high\n",
    "        rates_frame[-1][3] = new_low\n",
    "        rates_frame[-1][4] = new_close\n",
    "\n",
    "    df = pd.DataFrame(rates_frame, columns=['time', 'open', 'high', 'low', 'close'])\n",
    "    df['Date'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df['Date'] = df.Date.dt.strftime('%Y.%m.%d %H:%M:%S')\n",
    "    df = df.drop(['time'], axis=1)\n",
    "    df = df.reindex(columns=['Date', 'open', 'high', 'low', 'close'])\n",
    "    # convert to float to avoid sai so.\n",
    "    df.open = df.open.astype(float)\n",
    "    df.high = df.high.astype(float)\n",
    "    df.low = df.low.astype(float)\n",
    "    df.close = df.close.astype(float)\n",
    "    real_close = df.close.iat[-1]\n",
    "    real_high = df.high.iat[-1]\n",
    "    real_low = df.low.iat[-1]\n",
    "    df['ATR'] = talib.ATR(df.high, df.low, df.close)\n",
    "    df['RSI'] = talib.RSI(df.close)\n",
    "    df['MACD'], df['SIGNAL'], df['HIST'] = MACD(df.close)\n",
    "    nine_period_high = df['high'].rolling(window= 9).max()\n",
    "    nine_period_low = df['low'].rolling(window= 9).min()\n",
    "    df['tenkan_sen'] = (nine_period_high + nine_period_low) /2\n",
    "    # Kijun-sen (Base Line): (26-period high + 26-period low)/2))\n",
    "    period26_high = df['high'].rolling(window=26).max()\n",
    "    period26_low = df['low'].rolling(window=26).min()\n",
    "    df['kijun_sen'] = (period26_high + period26_low) / 2\n",
    "    # Senkou Span A (Leading Span A): (Conversion Line + Base Line)/2))\n",
    "    df['senkou_span_a'] = ((df['tenkan_sen'] + df['kijun_sen']) / 2).shift(26)\n",
    "    # Senkou Span B (Leading Span B): (52-period high + 52-period low)/2))\n",
    "    period52_high = df['high'].rolling(window=52).max()\n",
    "    period52_low = df['low'].rolling(window=52).min()\n",
    "    df['senkou_span_b'] = ((period52_high + period52_low) / 2).shift(26)\n",
    "    # The most current closing price plotted 26 time periods behind (optional)\n",
    "    df['chikou_span'] = df['close'].shift(-26)\n",
    "    # create a quick plot of the results to see what we have created\n",
    "        \n",
    "    x_signal = df.SIGNAL.values\n",
    "    \n",
    "    maxima = argrelextrema(np.array(x_signal), np.greater)[0]\n",
    "    maxima_val = [x_signal[max_val] for max_val in maxima]\n",
    "    minima = argrelextrema(np.array(x_signal), np.less)[0]\n",
    "    minima_val = [x_signal[min_val] for min_val in minima]\n",
    "\n",
    "    # calculate slope\n",
    "    tenkan_array = df.tenkan_sen[-5:]\n",
    "    close_array = df.close[-5:]\n",
    "    kinjun_array = df.kijun_sen[-5:]\n",
    "    macd_array = df.MACD[-5:]\n",
    "    signal_array = df.SIGNAL[-5:]\n",
    "    hist_array = df.HIST[-5:]\n",
    "\n",
    "    tenkan_slope = calculate_slope(tenkan_array)\n",
    "    close_slope = calculate_slope(close_array)\n",
    "    kinjun_slope = calculate_slope(kinjun_array)\n",
    "    macd_slope = calculate_slope(macd_array)\n",
    "    signal_slope = calculate_slope(signal_array)\n",
    "    hist_slope = calculate_slope(hist_array)\n",
    "    \n",
    "    input_df_1 = df['tenkan_sen'][-14:]\n",
    "    input_df_2 = df['kijun_sen'][-14:]\n",
    "    input_df_3 = df['senkou_span_a'][-14:]\n",
    "    input_df_4 = df['senkou_span_b'][-14:]\n",
    "    input_df_5 = df['HIST'][-14:]\n",
    "    input_df_6 = df['MACD'][-14:]\n",
    "    input_df_7 = df['SIGNAL'][-14:]\n",
    "    input_df_8 = df['close'][-14:]\n",
    "    input_df_9 = df['RSI'][-14:]\n",
    "    input_df_10 = df['ATR'][-14:]\n",
    "#     if df.Buy_Signal.iat[-1] != \"0\":\n",
    "#         print(signal_slope, price_slope, hist_slope)\n",
    "\n",
    "    normal_array = [calculate_slope(x) for x in [input_df_1, input_df_2, input_df_3, input_df_4, input_df_5, input_df_6, input_df_7, input_df_8, input_df_9, input_df_10]]\n",
    "\n",
    "    slope = 0\n",
    "    if hist_slope > 0 and macd_slope > 0 and close_slope > 0:\n",
    "#         if (len(minima_val) > 0 and minima_val[-1] < 0) or (len(minima_val) >= 2 and df.SIGNAL.iat[-1] > minima_val[-2]):\n",
    "            # detect minimal of signal\n",
    "        slope = 1\n",
    "\n",
    "    elif hist_slope < 0 and macd_slope < 0 and close_slope < 0:\n",
    "#         if (len(maxima_val) > 0 and maxima_val[-1] > 0) or (len(maxima_val) >= 2 and df.SIGNAL.iat[-1] < maxima_val[-2]):\n",
    "            # detect minimal of signal\n",
    "        slope = -1\n",
    "\n",
    "#     if timeframe != mt5.TIMEFRAME_H4:\n",
    "#         conditions = [\n",
    "#             (df['kijun_sen'] < df['close']) & (df['HIST'] > df['HIST'].shift()),\n",
    "#             (df['kijun_sen'] > df['close']) & (df['HIST'] < df['HIST'].shift())\n",
    "#         ]\n",
    "#     else:\n",
    "    conditions = [\n",
    "        (df['tenkan_sen'] > df['senkou_span_a']) & (df['tenkan_sen'] > df['senkou_span_b']) &\n",
    "        (df['kijun_sen'] < df['close']) & (df['HIST'] > df['HIST'].shift()),\n",
    "        (df['tenkan_sen'] < df['senkou_span_a']) & (df['tenkan_sen'] < df['senkou_span_b']) &\n",
    "        (df['kijun_sen'] > df['close']) & (df['HIST'] < df['HIST'].shift())\n",
    "    ]\n",
    "\n",
    "    values = ['Buy', 'Sell']\n",
    "    df['Buy_Signal'] = np.select(conditions, values)\n",
    "\n",
    "    conditions = [\n",
    "        (df['kijun_sen'] > df['close']),\n",
    "        (df['kijun_sen'] < df['close'])\n",
    "    ]\n",
    "    values = ['Close_Buy', 'Close_Sell']\n",
    "    df['Close_Signal'] = np.select(conditions, values)\n",
    "\n",
    "    mapping_timeframe = {\n",
    "        mt5.TIMEFRAME_H1: \"H1\",\n",
    "        mt5.TIMEFRAME_M15: \"M15\",\n",
    "        mt5.TIMEFRAME_M5: \"M5\",\n",
    "        mt5.TIMEFRAME_M30: \"M30\",\n",
    "        mt5.TIMEFRAME_D1: \"D1\",\n",
    "        mt5.TIMEFRAME_H4: \"H4\",\n",
    "        mt5.TIMEFRAME_H12: \"H12\",\n",
    "    }\n",
    "    map_timeframe = mapping_timeframe.get(timeframe)\n",
    "    \n",
    "    pivots = get_pivot(df)\n",
    "    resistance, support = None, None\n",
    "    if len(pivots) >= 2:\n",
    "        pivot_1 = pivots[-1][1]\n",
    "        pivot_2 = pivots[-2][1]\n",
    "        if pivot_1 > pivot_2:\n",
    "            resistance, support = pivot_1, pivot_2\n",
    "        else:\n",
    "            resistance, support = pivot_2, pivot_1\n",
    "    \n",
    "    if save_frame:\n",
    "        date_save = str(df.Date.iat[-1]).replace(\":\", \"\")\n",
    "        plot_chart(df, pivots, order_label, order_date, stop_loss, signal_slope, x_signal, minima, maxima)\n",
    "\n",
    "    return str(df.Date.iat[-1]), df.Buy_Signal.iat[-1], df.Close_Signal.iat[-1], real_close, real_high, real_low, df.kijun_sen.iat[-1], df['ATR'].iat[-1], normal_array, resistance, support, slope, 0, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T12:20:10.845385Z",
     "start_time": "2022-04-20T12:20:10.764023Z"
    }
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "\n",
    "def thread_mt5(symbol):\n",
    "    # reset flags\n",
    "    global X\n",
    "    global y\n",
    "    order_p = None\n",
    "    stop_loss = None\n",
    "    order_size = None\n",
    "    order_index = None\n",
    "    profit = 0\n",
    "    max_profit = 0\n",
    "    bet = 0.01\n",
    "    max_bet = 10\n",
    "    max_loss = 0\n",
    "    num_loss = 0\n",
    "    num_profit = 0\n",
    "    take_profit = None\n",
    "    all_profit = []\n",
    "    all_diff = []\n",
    "    with_ema = True\n",
    "    with_stoploss = True\n",
    "    with_dx = True\n",
    "    adx_threshold = 0\n",
    "    cost = 0.1\n",
    "    lock_back_frame = 14\n",
    "    max_diff = 0\n",
    "    min_diff = 0\n",
    "    threshold = 0.95\n",
    "    waiting_frame = 0\n",
    "    accept_next = \"\"\n",
    "    order_date = \"\"\n",
    "    max_diff_date = \"\"\n",
    "    prev_time = \"\"\n",
    "    all_max_diff = []\n",
    "    all_min_diff = []\n",
    "    x_train_max = []\n",
    "    all_slope = []\n",
    "    timezone = pytz.timezone(\"Europe/Athens\")\n",
    "    timeframe_1 = mt5.TIMEFRAME_H1\n",
    "    timeframe_2 = mt5.TIMEFRAME_H4\n",
    "    timeframe_3 = mt5.TIMEFRAME_H12\n",
    "    timeframe_4 = mt5.TIMEFRAME_M30\n",
    "    time_inremental = timedelta(hours=1)\n",
    "    current_time = datetime(2018, 10, 11, 0, 0, 0, tzinfo=timezone)\n",
    "#     stop_time = datetime(2017, 10, 10, 0, 0, 0, tzinfo=timezone)\n",
    "    \n",
    "    # current_time = datetime.utcnow() - timedelta(days=30)\n",
    "#     stop_time = datetime.utcnow() + timedelta(days=2)\n",
    "#     current_time = datetime.now(tz=timezone) - timedelta(days=5)\n",
    "    stop_time = datetime.now(tz=timezone) + timedelta(days=2)\n",
    "    print(current_time)\n",
    "    print(stop_time)\n",
    "    while current_time.timestamp() < stop_time.timestamp():\n",
    "        # print(current_time.minute)\n",
    "        df1date, h1_trend, close_signal_1, current_price_1, high_price_1, low_price_1, kijun_sen_1, atr_1, dftrain_1, resistance_1, support_1, slope_1, signal_slope_1, dataframe_1 = ichimoku_cloud(utc_to=current_time, timeframe=timeframe_1, symbol=symbol, save_frame=False, order_label=\"\", order_date=\"\", stop_loss=None)\n",
    "        df2date, h2_trend, close_signal_2, current_price_2, high_price_2, low_price_2, kijun_sen_2, atr_2, dftrain_2, resistance_2, support_2, slope_2, signal_slope_2, dataframe_2 = ichimoku_cloud(utc_to=current_time, timeframe=timeframe_2, symbol=symbol, save_frame=False, order_label=\"\", order_date=\"\", stop_loss=None)\n",
    "        df3date, h3_trend, close_signal_3, current_price_3, high_price_3, low_price_3, kijun_sen_3, atr_3, dftrain_3, resistance_3, support_3, slope_3, signal_slope_3, dataframe_3 = ichimoku_cloud(utc_to=current_time, timeframe=timeframe_3, symbol=symbol, save_frame=False, order_label=\"\", order_date=\"\", stop_loss=None)\n",
    "        #df4date, h4_trend, close_signal_4, current_price_4, high_price_4, low_price_4, kijun_sen_4, atr_4, dftrain_4, resistance_4, support_4, slope_4, signal_slope_4, dataframe_4 = ichimoku_cloud(utc_to=current_time, timeframe=timeframe_4, symbol=symbol, save_frame=False, order_label=\"\", order_date=\"\", stop_loss=None)\n",
    "\n",
    "        if df1date == prev_time:\n",
    "            current_time += time_inremental\n",
    "            continue\n",
    "\n",
    "        current_price = current_price_1\n",
    "        current_trend = \"0\"\n",
    "        if h1_trend == h2_trend == h3_trend == \"Sell\":\n",
    "            current_trend = \"Sell\"\n",
    "        elif h1_trend == h2_trend == h3_trend == \"Buy\":\n",
    "            current_trend = \"Buy\"\n",
    "        elif close_signal_1 == \"Close_Sell\":\n",
    "            current_trend = \"Close_Sell\"\n",
    "        elif close_signal_1 == \"Close_Buy\":\n",
    "            current_trend = \"Close_Buy\"\n",
    "            \n",
    "        if order_p and order_size == \"Sell\" and (current_trend == \"Neutral\" or current_trend == 'Buy' or h1_trend == \"Buy\" or h2_trend == \"Buy\" or current_trend == 'Close_Sell' or (stop_loss and stop_loss < high_price_1) or current_time.timestamp() > stop_time.timestamp()): \n",
    "            diff = order_p - current_price\n",
    "            if stop_loss and stop_loss < high_price_1:\n",
    "                diff = order_p - stop_loss\n",
    "\n",
    "            print(f\"{df1date} close \\x1B[31m{order_size}\\x1b[0m order at price {current_price}, max diff {round(max_diff, 5)} diff {round(diff, 5)}\")\n",
    "            ichimoku_cloud(utc_to=current_time, timeframe=timeframe_1, symbol=symbol, save_frame=True, order_label=diff, order_date=order_date, stop_loss=stop_loss)\n",
    "            #ichimoku_cloud(utc_to=current_time, timeframe=timeframe_2, symbol=symbol, save_frame=True, order_label=f\"Close Sell {round(diff, 5)}\", order_date=order_date)\n",
    "            profit += (diff)\n",
    "            \n",
    "            all_profit.append(profit)\n",
    "#             all_diff.append(diff)\n",
    "            if diff < 0:\n",
    "                all_min_diff.append(diff)\n",
    "                max_loss += diff\n",
    "                num_loss += 1\n",
    "                label = 0\n",
    "            elif diff > 0:\n",
    "                all_max_diff.append(max_diff)\n",
    "                max_profit += diff\n",
    "                num_profit += 1\n",
    "                label = 1  \n",
    "            # y.append(label)\n",
    "            # reset flags\n",
    "            order_p = None\n",
    "            order_size = None\n",
    "            stop_loss = None\n",
    "            take_profit = None\n",
    "            max_diff = 0\n",
    "            if current_time.timestamp() > stop_time.timestamp():\n",
    "                break\n",
    "        elif order_p and order_size == \"Buy\" and (current_trend == \"Neutral\" or current_trend == 'Sell' or h1_trend == \"Sell\" or h2_trend == \"Sell\" or current_trend == 'Close_Buy' or (stop_loss and stop_loss > low_price_1) or current_time.timestamp() > stop_time.timestamp()):\n",
    "            diff = current_price - order_p\n",
    "            if stop_loss and stop_loss > low_price_1:\n",
    "                diff = stop_loss - order_p\n",
    "            ichimoku_cloud(utc_to=current_time, timeframe=timeframe_1, symbol=symbol, save_frame=True, order_label=diff, order_date=order_date, stop_loss=stop_loss)\n",
    "            #ichimoku_cloud(utc_to=current_time, timeframe=timeframe_2, symbol=symbol, save_frame=True, order_label=f\"Close Buy {round(diff, 5)}\", order_date=order_date)\n",
    "            print(f\"{df1date} close \\x1b[48;5;4m{order_size}\\x1b[0m order at price {current_price}, max diff {round(max_diff, 5)} diff {round(diff, 5)}\")\n",
    "            profit += (diff)\n",
    "            \n",
    "            all_diff.append(diff)\n",
    "            all_profit.append(profit)\n",
    "            if diff < 0:\n",
    "                all_min_diff.append(diff)\n",
    "                max_loss += diff\n",
    "                num_loss += 1\n",
    "                label = 0\n",
    "            elif diff > 0:\n",
    "                all_max_diff.append(max_diff)\n",
    "                max_profit += diff\n",
    "                num_profit += 1\n",
    "                label = 1\n",
    "\n",
    "#             X.append(x_train_max)\n",
    "#             label = pyautogui.confirm(text='Are you sure?', title='Confirm delete', buttons=[\"1\", \"0\"])\n",
    "#             y.append(int(label))\n",
    "\n",
    "            # reset flags\n",
    "            order_p = None\n",
    "            order_size = None\n",
    "            stop_loss = None\n",
    "            take_profit = None\n",
    "            max_diff = 0\n",
    "            if current_time.timestamp() > stop_time.timestamp():\n",
    "                break\n",
    "        elif order_p is None and current_trend == \"Sell\" and slope_1 == -1 and support_1 and current_price < support_1:\n",
    "#             all_slope.append(signal_slope_1)\n",
    "            order_p = current_price\n",
    "            order_size = current_trend\n",
    "            order_date = df1date\n",
    "            stop_loss = kijun_sen_1\n",
    "            take_profit = None\n",
    "            max_diff = 0\n",
    "            accept_next = \"\"\n",
    "            print(f\"{df1date} new \\x1B[31m{order_size}\\x1b[0m order at price {current_price}\")\n",
    "#             ichimoku_cloud(utc_to=current_time, timeframe=timeframe_1, symbol=symbol, save_frame=True, order_label=f\"Sell {predict_1}\", order_date=df1date)\n",
    "\n",
    "        elif order_p is None and current_trend == \"Buy\" and slope_1 == 1 and resistance_1 and current_price > resistance_1:\n",
    "            all_slope.append(signal_slope_1)\n",
    "            order_p = current_price\n",
    "            order_size = current_trend\n",
    "            order_date = df1date\n",
    "            stop_loss = kijun_sen_1\n",
    "            take_profit = None\n",
    "            accept_next = \"\"\n",
    "            max_diff = 0\n",
    "            x_train_max = dataframe_1\n",
    "            print(f\"{df1date} new \\x1b[48;5;2m{order_size}\\x1b[0m order at price {current_price}\")\n",
    "            # ichimoku_cloud(utc_to=current_time, timeframe=timeframe_1, symbol=symbol, save_frame=True, order_label=f\"Buy {predict_1}\", order_date=df1date)\n",
    "            # clear_output(wait=True)\n",
    "\n",
    "        if order_size == \"Sell\":\n",
    "            diff = order_p - current_price\n",
    "            if diff > max_diff:\n",
    "                max_diff = diff\n",
    "            if stop_loss is None and diff > atr_1:\n",
    "                stop_loss = order_p\n",
    "            if stop_loss and stop_loss > kijun_sen_1 > current_price:\n",
    "                stop_loss = kijun_sen_1\n",
    "        elif order_size == \"Buy\":\n",
    "            diff = current_price - order_p\n",
    "            if diff > max_diff:\n",
    "                max_diff = diff \n",
    "                max_diff_date = current_time\n",
    "            if stop_loss is None and diff > atr_1:\n",
    "                stop_loss = order_p\n",
    "            if stop_loss and stop_loss < kijun_sen_1 < current_price:\n",
    "                stop_loss = kijun_sen_1\n",
    "\n",
    "        current_time += time_inremental\n",
    "        prev_time = df1date\n",
    "\n",
    "    print(f\"symbol {symbol}\")\n",
    "    print(f\"total trade: {len(all_profit)} avg loss: {round(max_loss/num_loss, 1)} / {num_loss} avg profit {round(max_profit/num_profit, 1)} / {num_profit}\")\n",
    "    print(f\"total loss: {round(max_loss, 5)} total profit {round(max_profit, 5)} real profit {round(profit, 5)}\")\n",
    "    print(f\"all max diff avg {sum(all_max_diff) / len(all_max_diff)}\")\n",
    "    print(f\"all min diff avg {sum(all_min_diff) / len(all_min_diff)}\")\n",
    "    print(f\"ROI { round((max_profit/num_profit)/(-max_loss/num_loss), 1) }\")\n",
    "    print(f\"winrate {round(num_profit/len(all_profit)*100, 1)}% total trade {len(all_profit)}\")\n",
    "    # Initialize figure with subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        column_widths=[2],\n",
    "        row_heights=[1, 0.5])\n",
    "    #     specs=[[{\"type\": \"scatter\", \"colspan\": 2}, {\"type\": \"scatter\"}],\n",
    "    #            [            None                    , {\"type\": \"bar\"}]])\n",
    "    # Add locations bar chart\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=['total profit', 'total loss'],  y=[max_profit, abs(max_loss)], showlegend=False, \n",
    "               text=[f\"avg profit {round(max_profit/num_profit, 2)} / {num_profit}\", \n",
    "                     f\"avg loss: {round(max_loss/num_loss, 2)} / {num_loss}\"], textposition='auto'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=all_profit, showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    # Set theme, margin, and annotation in layout\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_dark\",\n",
    "        margin=dict(r=100, t=25, b=50, l=60),\n",
    "        annotations=[\n",
    "            dict(\n",
    "                text=f\"H1 H4 atr1 kijun_sen_1 no neutral\",\n",
    "                showarrow=False,\n",
    "                xref=\"paper\",\n",
    "                yref=\"paper\",\n",
    "                x=0,\n",
    "                y=0)\n",
    "        ]\n",
    "    )\n",
    "    fig.show()\n",
    "    return all_diff, all_slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T12:44:59.696535Z",
     "start_time": "2022-04-20T12:20:12.004365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-07 11:07:11.936243+03:00\n",
      "2018.10.11 18:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1219.53\n",
      "2018.10.12 20:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1218.25, max diff 5.1 diff -1.28\n",
      "2018.10.15 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1221.94\n",
      "2018.10.15 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1230.68, max diff 9.84 diff 2.865\n",
      "2018.10.23 11:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1235.66\n",
      "2018.10.23 19:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1231.03, max diff 2.28 diff -5.74\n",
      "2018.10.26 10:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1236.43\n",
      "2018.10.26 15:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1233.34, max diff 0.95 diff -2.76\n",
      "2018.12.04 04:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1235.23\n",
      "2018.12.05 02:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1236.76, max diff 5.58 diff 0.755\n",
      "2018.12.06 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1243.95\n",
      "2018.12.06 19:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1239.62, max diff 0 diff -4.395\n",
      "2018.12.07 15:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1244.02\n",
      "2018.12.10 15:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1245.23, max diff 5.8 diff 1.245\n",
      "2018.12.19 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1255.75\n",
      "2018.12.19 20:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1251.65, max diff 0 diff -3.62\n",
      "2018.12.20 11:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1256.93\n",
      "2018.12.21 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1257.84, max diff 7.49 diff -0.48\n",
      "2018.12.24 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1266.33\n",
      "2018.12.26 19:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1269.87, max diff 11.86 diff 3.54\n",
      "2018.12.28 07:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1278.5\n",
      "2018.12.28 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1278.34, max diff 2.39 diff -2.06\n",
      "2019.01.02 10:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1288.08\n",
      "2019.01.02 15:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1284.29, max diff 0 diff -4.625\n",
      "2019.01.03 21:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1293.36\n",
      "2019.01.04 10:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1291.8, max diff 3.65 diff -1.79\n",
      "2019.01.09 19:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1289.93\n",
      "2019.01.10 18:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1289.11, max diff 6.98 diff -0.615\n",
      "2019.01.25 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1293.66\n",
      "2019.01.30 15:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1311.32, max diff 21.51 diff 17.42\n",
      "2019.01.30 22:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1318.92\n",
      "2019.02.01 01:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1321.08, max diff 6.44 diff 2.16\n",
      "2019.02.18 03:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1324.74\n",
      "2019.02.19 06:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1323.96, max diff 2.18 diff -0.595\n",
      "2019.02.19 11:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1329.14\n",
      "2019.02.20 21:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1341.12, max diff 17.36 diff 12.275\n",
      "2019.04.04 15:00:00 new \u001b[31mSell\u001b[0m order at price 1284.87\n",
      "2019.04.04 17:00:00 close \u001b[31mSell\u001b[0m order at price 1287.43, max diff 2.18 diff -2.875\n",
      "2019.04.15 10:00:00 new \u001b[31mSell\u001b[0m order at price 1287.03\n",
      "2019.04.15 18:00:00 close \u001b[31mSell\u001b[0m order at price 1288.68, max diff 2.09 diff -1.85\n",
      "2019.04.16 07:00:00 new \u001b[31mSell\u001b[0m order at price 1284.92\n",
      "2019.04.16 09:00:00 close \u001b[31mSell\u001b[0m order at price 1286.59, max diff 0.5 diff -1.415\n",
      "2019.04.16 12:00:00 new \u001b[31mSell\u001b[0m order at price 1283.64\n",
      "2019.04.18 10:00:00 close \u001b[31mSell\u001b[0m order at price 1275.27, max diff 11.81 diff 8.17\n",
      "2019.04.23 15:00:00 new \u001b[31mSell\u001b[0m order at price 1267.77\n",
      "2019.04.23 19:00:00 close \u001b[31mSell\u001b[0m order at price 1272.68, max diff 0 diff -4.225\n",
      "2019.05.31 18:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1301.78\n",
      "2019.06.04 15:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1323.27, max diff 26.64 diff 19.165\n",
      "2019.06.05 09:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1333.58\n",
      "2019.06.05 17:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1332.47, max diff 7.31 diff -1.53\n",
      "2019.06.13 09:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1337.86\n",
      "2019.06.13 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1336.23, max diff 0 diff -3.365\n",
      "2019.06.13 19:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1339.46\n",
      "2019.06.14 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1351.0, max diff 15.77 diff 6.505\n",
      "2019.06.19 21:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1352.2\n",
      "2019.06.21 09:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1392.49, max diff 53.84 diff 42.675\n",
      "2019.06.24 18:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1413.85\n",
      "2019.06.25 19:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1425.61, max diff 22.42 diff 8.025\n",
      "2019.07.17 22:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1426.41\n",
      "2019.07.18 15:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1417.51, max diff 1.37 diff -11.35\n",
      "2019.07.18 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1429.85\n",
      "2019.07.19 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1440.94, max diff 16.23 diff 3.96\n",
      "2019.08.05 05:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1451.05\n",
      "2019.08.06 07:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1463.1, max diff 23.02 diff 6.105\n",
      "2019.08.07 02:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1476.85\n",
      "2019.08.08 11:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1496.69, max diff 31.17 diff 19.055\n",
      "2019.08.13 09:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1522.44\n",
      "2019.08.13 15:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1520.15, max diff 9.85 diff -4.965\n",
      "2019.08.15 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1523.89\n",
      "2019.08.16 05:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1517.8, max diff 2.71 diff -5.925\n",
      "2019.08.23 17:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1519.2\n",
      "2019.08.26 19:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1529.13, max diff 28.3 diff 9.275\n",
      "2019.08.27 17:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1538.26\n",
      "2019.08.28 05:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1534.81, max diff 5.48 diff -2.93\n",
      "2019.09.03 17:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1544.49\n",
      "2019.09.04 10:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1536.57, max diff 4.62 diff -7.695\n",
      "2019.09.04 19:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1547.54\n",
      "2019.09.05 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1542.79, max diff 8.06 diff -2.05\n",
      "2019.09.30 12:00:00 new \u001b[31mSell\u001b[0m order at price 1488.0\n",
      "2019.10.01 17:00:00 close \u001b[31mSell\u001b[0m order at price 1483.64, max diff 26.48 diff 14.265\n",
      "2019.10.11 14:00:00 new \u001b[31mSell\u001b[0m order at price 1482.86\n",
      "2019.10.14 01:00:00 close \u001b[31mSell\u001b[0m order at price 1484.13, max diff 4.7 diff -5.765\n",
      "2019.10.15 20:00:00 new \u001b[31mSell\u001b[0m order at price 1480.5\n",
      "2019.10.16 15:00:00 close \u001b[31mSell\u001b[0m order at price 1490.68, max diff 0 diff -7.395\n",
      "2019.10.31 12:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1505.33\n",
      "2019.11.01 15:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1508.01, max diff 7.42 diff 2.27\n",
      "2019.11.07 17:00:00 new \u001b[31mSell\u001b[0m order at price 1469.77\n",
      "2019.11.11 09:00:00 close \u001b[31mSell\u001b[0m order at price 1464.82, max diff 11.44 diff 5.15\n",
      "2019.11.11 17:00:00 new \u001b[31mSell\u001b[0m order at price 1449.03\n",
      "2019.11.12 18:00:00 close \u001b[31mSell\u001b[0m order at price 1453.6, max diff 0 diff -5.425\n",
      "2019.11.29 15:00:00 new \u001b[31mSell\u001b[0m order at price 1453.53\n",
      "2019.11.29 16:00:00 close \u001b[31mSell\u001b[0m order at price 1457.17, max diff 0 diff -2.625\n",
      "2019.12.24 17:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1498.64\n",
      "2019.12.27 08:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1508.78, max diff 14.89 diff 9.53\n",
      "2019.12.31 03:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1521.28\n",
      "2019.12.31 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1518.76, max diff 3.24 diff -3.23\n",
      "2020.01.02 15:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1528.1\n",
      "2020.01.02 19:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1525.29, max diff 1.07 diff -4.095\n",
      "2020.01.03 03:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1534.98\n",
      "2020.01.06 17:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1564.24, max diff 43.87 diff 29.465\n",
      "2020.01.08 01:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1595.06\n",
      "2020.01.08 10:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1581.0, max diff 3.95 diff -9.845\n",
      "2020.01.27 03:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1581.14\n",
      "2020.01.27 20:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1578.23, max diff 4.33 diff -2.91\n",
      "2020.01.30 19:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1584.86\n",
      "2020.01.30 21:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1579.84, max diff 0 diff -7.465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.01.31 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1582.95\n",
      "2020.02.03 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1582.85, max diff 6.47 diff -0.77\n",
      "2020.02.14 13:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1576.36\n",
      "2020.02.14 15:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1580.17, max diff 0 diff -1.28\n",
      "2020.02.14 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1582.57\n",
      "2020.02.17 10:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1580.34, max diff 1.33 diff -3.11\n",
      "2020.02.18 02:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1586.42\n",
      "2020.02.20 03:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1609.25, max diff 26.48 diff 19.82\n",
      "2020.02.20 13:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1617.55\n",
      "2020.02.24 21:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1653.73, max diff 69.35 diff 46.28\n",
      "2020.03.05 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1658.43\n",
      "2020.03.06 17:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1663.97, max diff 29.45 diff 9.545\n",
      "2020.03.09 01:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1698.06\n",
      "2020.03.09 07:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1662.55, max diff 2.84 diff -25.505\n",
      "2020.03.13 18:00:00 new \u001b[31mSell\u001b[0m order at price 1534.79\n",
      "2020.03.16 01:00:00 close \u001b[31mSell\u001b[0m order at price 1545.84, max diff 22.37 diff -16.45\n",
      "2020.03.16 13:00:00 new \u001b[31mSell\u001b[0m order at price 1477.82\n",
      "2020.03.16 23:00:00 close \u001b[31mSell\u001b[0m order at price 1514.85, max diff 22.17 diff -35.405\n",
      "2020.04.02 17:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1608.26\n",
      "2020.04.03 23:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1615.59, max diff 14.64 diff 6.85\n",
      "2020.04.06 10:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1631.32\n",
      "2020.04.07 10:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1645.72, max diff 34.5 diff 13.48\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"images/1\"):\n",
    "    os.remove(f\"images/1/{file}\")\n",
    "for file in os.listdir(\"images/0\"):\n",
    "    os.remove(f\"images/0/{file}\")\n",
    "all_diff, all_slope = thread_mt5(\"XAUUSD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T16:22:19.306912Z",
     "start_time": "2022-04-04T16:22:19.179912Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X, train_y = [], []\n",
    "number_timestep = 50\n",
    "idx = 0\n",
    "for value_x, value_y in zip(X, y):\n",
    "    scaler = MinMaxScaler()\n",
    "    x_trans = scaler.fit_transform(value_x[['MACD','SIGNAL','HIST']][-number_timestep:])\n",
    "    print(x_trans.shape)\n",
    "    train_X.append(x_trans)\n",
    "    train_y.append(value_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T17:22:24.553807Z",
     "start_time": "2022-04-20T12:44:59.774533Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-06 10:49:13.349755+03:00\n",
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-06 10:49:13.363744+03:00\n",
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-06 10:49:13.390973+03:00\n",
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-06 10:49:13.423867+03:00\n",
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-06 10:49:13.443667+03:00\n",
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-06 10:49:13.470660+03:00\n",
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-06 10:49:13.517630+03:00\n",
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-06 10:49:13.562704+03:00\n",
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-06 10:49:13.590249+03:00\n",
      "2018-10-11 00:00:00+01:35\n",
      "2022-05-06 10:49:13.698473+03:00\n",
      "2018.10.11 00:00:00 new \u001b[31mSell\u001b[0m order at price 79.398\n",
      "2018.10.11 00:00:00 new \u001b[31mSell\u001b[0m order at price 6464.06\n",
      "2018.10.11 04:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 6.9415700000000005\n",
      "2018.10.11 12:00:00 close \u001b[31mSell\u001b[0m order at price 79.801, max diff 0.075 diff -0.514\n",
      "2018.10.11 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 6.90915, max diff 0 diff -0.02489\n",
      "2018.10.15 08:00:00 close \u001b[31mSell\u001b[0m order at price 6422.64, max diff 309.36 diff 124.435\n",
      "2018.10.17 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 0.99527\n",
      "2018.10.18 20:00:00 new \u001b[31mSell\u001b[0m order at price 1.14512\n",
      "2018.10.19 16:00:00 close \u001b[31mSell\u001b[0m order at price 1.15197, max diff 0.00022 diff -0.0076\n",
      "2018.10.18 04:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 6.93572\n",
      "2018.10.19 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 6.92982, max diff 0.01319 diff -0.00719\n",
      "2018.10.22 20:00:00 new \u001b[31mSell\u001b[0m order at price 1.14645\n",
      "2018.10.23 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 0.99594, max diff 0.00169 diff -0.00113\n",
      "2018.10.23 12:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1237.94\n",
      "2018.10.24 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 0.99774\n",
      "2018.10.23 04:00:00 new \u001b[31mSell\u001b[0m order at price 0.70616\n",
      "2018.10.25 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 0.99633, max diff 7e-05 diff -0.00138\n",
      "2018.10.24 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1229.35, max diff 0 diff -8.71\n",
      "2018.10.25 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.00009\n",
      "2018.10.24 04:00:00 close \u001b[31mSell\u001b[0m order at price 0.71031, max diff 0 diff -0.00416\n",
      "2018.10.26 00:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.00065, max diff 0 diff -0.00229\n",
      "2018.10.26 12:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.0024\n",
      "2018.10.24 20:00:00 new \u001b[31mSell\u001b[0m order at price 0.70614\n",
      "2018.10.26 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 0.99837, max diff 0 diff -0.00419\n",
      "2018.10.24 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 6.95395\n",
      "2018.10.26 16:00:00 close \u001b[31mSell\u001b[0m order at price 0.7091, max diff 0.00298 diff -0.00116\n",
      "2018.10.29 12:00:00 close \u001b[31mSell\u001b[0m order at price 1.13862, max diff 0.01197 diff 0.00497\n",
      "2018.10.26 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 6.95788, max diff 0.02 diff -0.00085\n",
      "2018.10.26 08:00:00 new \u001b[31mSell\u001b[0m order at price 78.788\n",
      "2018.10.30 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.00454\n",
      "2018.10.26 16:00:00 close \u001b[31mSell\u001b[0m order at price 79.386, max diff 0 diff -0.651\n",
      "2018.10.31 16:00:00 new \u001b[31mSell\u001b[0m order at price 1.13134\n",
      "2018.10.29 20:00:00 new \u001b[31mSell\u001b[0m order at price 0.70563\n",
      "2018.10.30 00:00:00 close \u001b[31mSell\u001b[0m order at price 0.70688, max diff 0 diff -0.00086\n",
      "2018.11.01 08:00:00 close \u001b[31mSell\u001b[0m order at price 1.13784, max diff 0 diff -0.00482\n",
      "2018.11.01 08:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.00291, max diff 0.00385 diff -0.00162\n",
      "2018.11.01 20:00:00 new \u001b[31mSell\u001b[0m order at price 63.53\n",
      "2018.10.29 12:00:00 new \u001b[31mSell\u001b[0m order at price 6248.9\n",
      "2018.10.29 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 6.97472\n",
      "2018.10.31 16:00:00 close \u001b[31mSell\u001b[0m order at price 6304.46, max diff 13.94 diff -80.79\n",
      "2018.11.01 08:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 6.94622, max diff 0.00245 diff -0.01023\n",
      "2018.11.05 04:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 113.25\n",
      "2018.11.07 00:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 113.555, max diff 0.23 diff -0.2185\n",
      "2018.11.12 08:00:00 new \u001b[31mSell\u001b[0m order at price 1.12567\n",
      "2018.11.12 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.01085\n",
      "2018.11.12 08:00:00 close \u001b[31mSell\u001b[0m order at price 60.93, max diff 3.63 diff 2.13\n",
      "2018.11.14 16:00:00 close \u001b[31mSell\u001b[0m order at price 1.12935, max diff 0.00393 diff -0.00745\n",
      "2018.11.14 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.00742, max diff 0 diff -0.00404\n",
      "2018.11.13 20:00:00 new \u001b[31mSell\u001b[0m order at price 55.36\n",
      "2018.11.09 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.31559\n",
      "2018.11.12 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 6.96465\n",
      "2018.11.13 08:00:00 new \u001b[31mSell\u001b[0m order at price 6277.96\n",
      "2018.11.16 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 0.73348\n",
      "2018.11.14 00:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 6.94862, max diff 0.00227 diff -0.02396\n",
      "2018.11.19 04:00:00 close \u001b[31mSell\u001b[0m order at price 57.25, max diff 0 diff -1.89\n",
      "2018.11.20 20:00:00 new \u001b[31mSell\u001b[0m order at price 53.32\n",
      "2018.11.21 16:00:00 close \u001b[31mSell\u001b[0m order at price 55.3, max diff 0 diff -2.14\n",
      "2018.11.20 08:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 0.72596, max diff 0 diff -0.00719\n",
      "2018.11.15 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.32022, max diff 0.00927 diff 0.00628\n",
      "2018.11.23 12:00:00 new \u001b[31mSell\u001b[0m order at price 50.79\n",
      "2018.11.27 16:00:00 new \u001b[31mSell\u001b[0m order at price 1.12853\n",
      "2018.11.28 16:00:00 close \u001b[31mSell\u001b[0m order at price 1.13441, max diff 0.00046 diff -0.00652\n",
      "2018.11.20 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.327142018.11.30 16:00:00 new \u001b[31mSell\u001b[0m order at price 1.13148\n",
      "\n",
      "2018.11.29 16:00:00 close \u001b[31mSell\u001b[0m order at price 51.96, max diff 0.91 diff -0.75\n",
      "2018.11.22 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.32297, max diff 0.00384 diff -0.00489\n",
      "2018.12.03 00:00:00 close \u001b[31mSell\u001b[0m order at price 1.13387, max diff 0 diff -0.00196\n",
      "2018.12.03 12:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1232.12\n",
      "2018.11.28 04:00:00 close \u001b[31mSell\u001b[0m order at price 3976.76, max diff 2639.21 diff 2223.775\n",
      "2018.12.03 04:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 0.73664\n",
      "2018.11.27 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.33223\n",
      "2018.11.28 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 82.986\n",
      "2018.12.05 00:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 0.73172, max diff 0.00246 diff -0.0059\n",
      "2018.11.28 20:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.32767, max diff 0.00072 diff -0.00514\n",
      "2018.12.03 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 113.659\n",
      "2018.12.03 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 113.494, max diff 0 diff -0.047\n",
      "2018.12.10 20:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1244.36, max diff 17.43 diff 10.005\n",
      "2018.12.03 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 113.637\n",
      "2018.12.04 00:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 113.318, max diff 0 diff -0.025\n",
      "2018.12.11 08:00:00 new \u001b[31mSell\u001b[0m order at price 51.07\n",
      "2018.12.11 16:00:00 close \u001b[31mSell\u001b[0m order at price 51.83, max diff 0 diff -1.33\n",
      "2018.12.13 00:00:00 new \u001b[31mSell\u001b[0m order at price 51.61\n",
      "2018.12.13 16:00:00 close \u001b[31mSell\u001b[0m order at price 52.04, max diff 0.68 diff -0.855\n",
      "2018.12.14 20:00:00 new \u001b[31mSell\u001b[0m order at price 51.41\n",
      "2018.12.04 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 82.794, max diff 0.786 diff 0.0755\n",
      "2018.12.07 04:00:00 new \u001b[31mSell\u001b[0m order at price 3400.24\n",
      "2018.12.06 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.33886\n",
      "2018.12.17 12:00:00 close \u001b[31mSell\u001b[0m order at price 51.72, max diff 0 diff -0.595\n",
      "2018.12.17 16:00:00 new \u001b[31mSell\u001b[0m order at price 50.62\n",
      "2018.12.20 12:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1256.92\n",
      "2018.12.07 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.32978, max diff 0.0046 diff -0.00862\n",
      "2018.12.13 12:00:00 close \u001b[31mSell\u001b[0m order at price 3390.0, max diff 154.35 diff 6.37\n",
      "2018.12.20 00:00:00 new \u001b[31mSell\u001b[0m order at price 0.71096\n",
      "2018.12.20 12:00:00 close \u001b[31mSell\u001b[0m order at price 0.71311, max diff 0.00213 diff -0.00351\n",
      "2018.12.21 16:00:00 new \u001b[31mSell\u001b[0m order at price 0.70526\n",
      "2018.12.26 16:00:00 close \u001b[31mSell\u001b[0m order at price 45.68, max diff 8.11 diff 5.385\n",
      "2018.12.20 20:00:00 new \u001b[31mSell\u001b[0m order at price 111.286\n",
      "2019.01.04 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1287.76, max diff 40.09 diff 30.355\n",
      "2018.12.28 12:00:00 close \u001b[31mSell\u001b[0m order at price 0.70556, max diff 0.00303 diff -0.00159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018.12.18 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.34844\n",
      "2018.12.19 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.34483, max diff 0 diff -0.00667\n",
      "2018.12.20 16:00:00 new \u001b[31mSell\u001b[0m order at price 79.075\n",
      "2019.01.10 00:00:00 new \u001b[31mSell\u001b[0m order at price 0.97283\n",
      "2018.12.26 20:00:00 close \u001b[31mSell\u001b[0m order at price 111.362, max diff 0.916 diff -0.116\n",
      "2019.01.10 16:00:00 close \u001b[31mSell\u001b[0m order at price 0.98294, max diff 0.00012 diff -0.0084\n",
      "2018.12.21 12:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.35415\n",
      "2019.01.02 16:00:00 new \u001b[31mSell\u001b[0m order at price 0.69984\n",
      "2019.01.03 00:00:00 close \u001b[31mSell\u001b[0m order at price 0.69364, max diff 0.00144 diff 0.0062\n",
      "2018.12.31 20:00:00 new \u001b[31mSell\u001b[0m order at price 109.601\n",
      "2018.12.31 16:00:00 new \u001b[31mSell\u001b[0m order at price 6.86843\n",
      "2019.01.02 16:00:00 close \u001b[31mSell\u001b[0m order at price 6.878, max diff 0.00403 diff -0.0114\n",
      "2019.01.03 12:00:00 close \u001b[31mSell\u001b[0m order at price 107.747, max diff 2.633 diff 1.6375\n",
      "2019.01.04 08:00:00 new \u001b[31mSell\u001b[0m order at price 6.87645\n",
      "2019.01.04 12:00:00 close \u001b[31mSell\u001b[0m order at price 6.87906, max diff 0 diff -0.00021\n",
      "2019.01.04 20:00:00 new \u001b[31mSell\u001b[0m order at price 6.86392\n",
      "2018.12.31 08:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.36133, max diff 0.01003 diff 0.00716\n",
      "2019.01.03 00:00:00 close \u001b[31mSell\u001b[0m order at price 74.591, max diff 3.017 diff 4.484\n",
      "2019.01.03 04:00:00 new \u001b[31mSell\u001b[0m order at price 74.263\n",
      "2019.01.03 08:00:00 close \u001b[31mSell\u001b[0m order at price 74.959, max diff 0 diff -0.233\n",
      "2019.01.25 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1297.65\n",
      "2019.01.15 20:00:00 close \u001b[31mSell\u001b[0m order at price 6.77267, max diff 0.12402 diff 0.07721\n",
      "2019.02.06 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.00244\n",
      "2019.02.08 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 0.99979, max diff 0.00024 diff -0.00282\n",
      "2019.02.04 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1312.8, max diff 27.71 diff 16.58\n",
      "2019.02.11 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.00487\n",
      "2019.02.11 20:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.00413, max diff 0 diff -0.00065\n",
      "2019.02.12 12:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.00905\n",
      "2019.02.13 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.00713, max diff 0 diff -0.00468\n",
      "2019.02.13 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.00931\n",
      "2019.02.15 00:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.00579, max diff 0 diff -0.00494\n",
      "2019.01.25 16:00:00 new \u001b[31mSell\u001b[0m order at price 6.75306\n",
      "2019.02.14 00:00:00 new \u001b[31mSell\u001b[0m order at price 1.12694\n",
      "2019.02.14 12:00:00 close \u001b[31mSell\u001b[0m order at price 1.13025, max diff 0.0003 diff -0.00306\n",
      "2019.01.28 04:00:00 new \u001b[31mSell\u001b[0m order at price 3415.47\n",
      "2019.02.18 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1324.63\n",
      "2019.02.01 00:00:00 close \u001b[31mSell\u001b[0m order at price 6.73346, max diff 0.04552 diff 0.01957\n",
      "2019.01.31 00:00:00 close \u001b[31mSell\u001b[0m order at price 3448.0, max diff 33.41 diff -41.72\n",
      "2019.02.21 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1327.59, max diff 20.61 diff 4.875\n",
      "2019.02.21 04:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 57.39\n",
      "2019.02.06 04:00:00 new \u001b[31mSell\u001b[0m order at price 3366.39\n",
      "2019.01.31 00:00:00 new \u001b[31mSell\u001b[0m order at price 1.31389\n",
      "2019.03.07 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.01134\n",
      "2019.02.11 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 110.455\n",
      "2019.02.25 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 56.01, max diff 0.4 diff -0.59\n",
      "2019.02.08 08:00:00 close \u001b[31mSell\u001b[0m order at price 3378.25, max diff 14.64 diff -18.86\n",
      "2019.03.07 16:00:00 new \u001b[31mSell\u001b[0m order at price 1.12253\n",
      "2019.02.15 00:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 110.343, max diff 0.572 diff -0.0575\n",
      "2019.03.01 04:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 57.71\n",
      "2019.03.01 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 55.99, max diff 0 diff -1.12\n",
      "2019.03.12 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.00671, max diff 0.00035 diff -0.00377\n",
      "2019.03.11 12:00:00 close \u001b[31mSell\u001b[0m order at price 1.12367, max diff 0.00368 diff -0.00321\n",
      "2019.02.06 04:00:00 close \u001b[31mSell\u001b[0m order at price 1.31516, max diff 0.00574 diff -6e-05\n",
      "2019.02.22 20:00:00 new \u001b[31mSell\u001b[0m order at price 6.70082\n",
      "2019.03.12 12:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 57.66\n",
      "2019.02.25 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 111.061\n",
      "2019.02.26 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 110.787, max diff 0 diff -0.198\n",
      "2019.02.28 16:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 111.328\n",
      "2019.02.28 20:00:00 close \u001b[31mSell\u001b[0m order at price 6.70272, max diff 0.02187 diff 0.0029\n",
      "2019.02.20 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 79.543\n",
      "2019.02.21 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 78.686, max diff 0 diff -0.586\n",
      "2019.03.28 16:00:00 new \u001b[31mSell\u001b[0m order at price 1.12274\n",
      "2019.03.20 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 58.71, max diff 2.06 diff 1.27\n",
      "2019.03.21 04:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 60.28\n",
      "2019.03.22 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 59.26, max diff 0 diff -0.89\n",
      "2019.02.25 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 79.135\n",
      "2019.03.07 00:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 111.631, max diff 0.665 diff 0.251\n",
      "2019.02.25 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 79.114, max diff 0 diff -0.0585\n",
      "2019.02.25 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 79.297\n",
      "2019.04.03 04:00:00 close \u001b[31mSell\u001b[0m order at price 1.12228, max diff 0.00304 diff 0.00018\n",
      "2019.02.27 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 79.201, max diff 0.455 diff -0.2205\n",
      "2019.04.01 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 60.48\n",
      "2019.03.15 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 111.856\n",
      "2019.03.15 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 111.515, max diff 0 diff -0.402\n",
      "2019.04.17 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.01068\n",
      "2019.03.21 00:00:00 new \u001b[31mSell\u001b[0m order at price 6.68226\n",
      "2019.03.21 16:00:00 close \u001b[31mSell\u001b[0m order at price 6.71225, max diff 0 diff -0.01542\n",
      "2019.04.16 12:00:00 new \u001b[31mSell\u001b[0m order at price 1279.16\n",
      "2019.03.18 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 3978.25\n",
      "2019.04.12 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 63.89, max diff 4.12 diff 3.41\n",
      "2019.04.26 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.01887, max diff 0.01091 diff 0.0084\n",
      "2019.04.23 16:00:00 new \u001b[31mSell\u001b[0m order at price 1.12135\n",
      "2019.03.21 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 3959.95, max diff 55.63 diff -31.415\n",
      "2019.04.24 08:00:00 close \u001b[31mSell\u001b[0m order at price 1273.62, max diff 11.39 diff 6.085\n",
      "2019.04.29 20:00:00 close \u001b[31mSell\u001b[0m order at price 1.11847, max diff 0.00841 diff 0.00274\n",
      "2019.04.22 04:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 65.73\n",
      "2019.03.29 12:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 4076.76\n",
      "2019.04.25 20:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 65.07, max diff 0.53 diff -0.655\n",
      "2019.04.17 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 0.71997\n",
      "2019.04.18 08:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 0.71638, max diff 0 diff -0.00388\n",
      "2019.04.17 12:00:00 new \u001b[31mSell\u001b[0m order at price 6.68279\n",
      "2019.04.18 08:00:00 close \u001b[31mSell\u001b[0m order at price 6.70253, max diff 0.00657 diff -0.02011\n",
      "2019.04.11 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 5053.13, max diff 1233.49 diff 1091.1\n",
      "2019.05.23 12:00:00 new \u001b[31mSell\u001b[0m order at price 1.11356\n",
      "2019.05.23 16:00:00 close \u001b[31mSell\u001b[0m order at price 1.11822, max diff 0 diff -0.00221\n",
      "2019.05.02 16:00:00 new \u001b[31mSell\u001b[0m order at price 0.70032\n",
      "2019.06.03 04:00:00 new \u001b[31mSell\u001b[0m order at price 0.9978\n",
      "2019.05.07 04:00:00 close \u001b[31mSell\u001b[0m order at price 0.70395, max diff 0.00383 diff -0.00086\n",
      "2019.04.12 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 80.123\n",
      "2019.06.06 12:00:00 close \u001b[31mSell\u001b[0m order at price 0.99122, max diff 0.01088 diff 0.00184\n",
      "2019.06.07 16:00:00 new \u001b[31mSell\u001b[0m order at price 0.98651\n",
      "2019.06.10 04:00:00 close \u001b[31mSell\u001b[0m order at price 0.99109, max diff 0 diff -0.00458\n",
      "2019.05.13 16:00:00 new \u001b[31mSell\u001b[0m order at price 0.69433\n",
      "2019.04.23 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 5539.26\n",
      "2019.04.24 08:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 5479.75, max diff 52.0 diff -127.885\n",
      "2019.04.18 08:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 80.138, max diff 0.497 diff -0.0335\n",
      "2019.05.06 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 6.81674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.05.20 00:00:00 close \u001b[31mSell\u001b[0m order at price 0.69139, max diff 0.0075 diff 0.00294\n",
      "2019.05.06 20:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 6.77292, max diff 0 diff -0.04722\n",
      "2019.05.31 12:00:00 new \u001b[31mSell\u001b[0m order at price 55.4\n",
      "2019.05.08 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 6.80758\n",
      "2019.06.20 12:00:00 new \u001b[31mSell\u001b[0m order at price 0.98471\n",
      "2019.05.03 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 5675.0\n",
      "2019.04.24 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.34387\n",
      "2019.06.06 20:00:00 close \u001b[31mSell\u001b[0m order at price 53.14, max diff 4.01 diff 2.18\n",
      "2019.06.26 20:00:00 close \u001b[31mSell\u001b[0m order at price 0.9779, max diff 0.01504 diff 0.00681\n",
      "2019.06.20 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1371.41\n",
      "2019.04.29 20:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.34571, max diff 0.00652 diff 0.00022\n",
      "2019.05.21 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 6.92824, max diff 0.13974 diff 0.11372\n",
      "2019.05.06 00:00:00 new \u001b[31mSell\u001b[0m order at price 76.828\n",
      "2019.05.07 04:00:00 close \u001b[31mSell\u001b[0m order at price 77.897, max diff 0 diff -0.919\n",
      "2019.06.26 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1409.46, max diff 64.62 diff 33.635\n",
      "2019.05.17 04:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 7199.19, max diff 2658.0 diff 1633.0\n",
      "2019.05.09 04:00:00 new \u001b[31mSell\u001b[0m order at price 76.676\n",
      "2019.05.10 04:00:00 close \u001b[31mSell\u001b[0m order at price 76.683, max diff 0.179 diff -0.5495\n",
      "2019.05.07 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.34728\n",
      "2019.05.08 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.34635, max diff 0 diff -0.00227\n",
      "2019.05.08 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.34774\n",
      "2019.05.10 00:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.34527, max diff 0.00167 diff -0.00215\n",
      "2019.06.14 20:00:00 new \u001b[31mSell\u001b[0m order at price 0.687\n",
      "2019.05.13 16:00:00 new \u001b[31mSell\u001b[0m order at price 75.78\n",
      "2019.05.29 08:00:00 new \u001b[31mSell\u001b[0m order at price 109.256\n",
      "2019.05.29 20:00:00 close \u001b[31mSell\u001b[0m order at price 109.589, max diff 0 diff -0.405\n",
      "2019.05.31 04:00:00 new \u001b[31mSell\u001b[0m order at price 109.015\n",
      "2019.06.19 20:00:00 close \u001b[31mSell\u001b[0m order at price 0.68834, max diff 0.00311 diff -0.00053\n",
      "2019.05.27 00:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 8885.99\n",
      "2019.05.20 00:00:00 close \u001b[31mSell\u001b[0m order at price 76.207, max diff 0.391 diff -0.066\n",
      "2019.05.17 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.34883\n",
      "2019.05.17 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.3462100000000001, max diff 0.00195 diff -0.00333\n",
      "2019.06.07 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 6.95278\n",
      "2019.06.07 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 6.94698, max diff 0 diff -0.01435\n",
      "2019.05.30 20:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 8451.99, max diff 0 diff -381.375\n",
      "2019.06.06 20:00:00 close \u001b[31mSell\u001b[0m order at price 108.385, max diff 1.097 diff 0.6525\n",
      "2019.07.23 16:00:00 new \u001b[31mSell\u001b[0m order at price 1.11506\n",
      "2019.06.10 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 6.954\n",
      "2019.07.25 16:00:00 close \u001b[31mSell\u001b[0m order at price 1.11335, max diff 0.00215 diff -0.00328\n",
      "2019.06.11 00:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 6.93633, max diff 0 diff -0.01516\n",
      "2019.07.18 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1446.08\n",
      "2019.07.19 16:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1426.01, max diff 0 diff -19.55\n",
      "2019.08.01 00:00:00 new \u001b[31mSell\u001b[0m order at price 1.10415\n",
      "2019.08.01 20:00:00 close \u001b[31mSell\u001b[0m order at price 1.10848, max diff 0.00118 diff -0.0053\n",
      "2019.05.31 08:00:00 new \u001b[31mSell\u001b[0m order at price 75.254\n",
      "2019.05.29 08:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.3508200000000001\n",
      "2019.05.30 12:00:00 close \u001b[48;5;4mBuy\u001b[0m order at price 1.34943, max diff 0.00116 diff -0.00201\n",
      "2019.06.20 00:00:00 new \u001b[31mSell\u001b[0m order at price 107.642\n",
      "2019.08.12 20:00:00 new \u001b[31mSell\u001b[0m order at price 0.96925\n",
      "2019.05.31 04:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 1.35388\n",
      "2019.08.13 16:00:00 close \u001b[31mSell\u001b[0m order at price 0.97531, max diff 0.00227 diff -0.00358\n",
      "2019.06.04 16:00:00 close \u001b[31mSell\u001b[0m order at price 75.661, max diff 0.167 diff -0.313\n",
      "2019.06.14 20:00:00 new \u001b[48;5;2mBuy\u001b[0m order at price 8461.75\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"images/1\"):\n",
    "    os.remove(f\"images/1/{file}\")\n",
    "for file in os.listdir(\"images/0\"):\n",
    "    os.remove(f\"images/0/{file}\")\n",
    "X_raw = []\n",
    "y_raw = []\n",
    "X = []\n",
    "y = []\n",
    "y_diff = []\n",
    "threads = []\n",
    "for symbol in [\"XAUUSD\", \"USDCHF\", \"AUDUSD\", \"AUDJPY\", \"USDJPY\", \"USDCAD\", \"BTCUSD\", \"USDCNH\", \"SpotCrude\", \"EURUSD\"]:\n",
    "    test_ = threading.Thread(target=thread_mt5, args=(symbol,), daemon=True)\n",
    "    threads.append(test_)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:38:14.505894Z",
     "start_time": "2022-02-23T16:38:14.470886Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"total trade: {len(all_profit)} avg loss: {round(max_loss/num_loss, 1)} / {num_loss} avg profit {round(max_profit/num_profit, 1)} / {num_profit}\")\n",
    "print(f\"total loss: {round(max_loss, 1)} total profit {max_profit} real profit {round(profit, 1)}\")\n",
    "print(f\"all max diff avg {sum(all_max_diff) / len(all_max_diff)}\")\n",
    "print(f\"all min diff avg {sum(all_min_diff) / len(all_min_diff)}\")\n",
    "print(f\"ROI { round((max_profit/num_profit)/(-max_loss/num_loss), 1) }\")\n",
    "print(f\"winrate {round(len(all_max_diff)/len(all_profit) * 100, 1)}% total trade {len(all_profit)}\")\n",
    "\n",
    "# Initialize figure with subplots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    column_widths=[2],\n",
    "    row_heights=[1, 0.5])\n",
    "#     specs=[[{\"type\": \"scatter\", \"colspan\": 2}, {\"type\": \"scatter\"}],\n",
    "#            [            None                    , {\"type\": \"bar\"}]])\n",
    "# Add locations bar chart\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['total profit', 'total loss'],  y=[max_profit, abs(max_loss)], showlegend=False, \n",
    "           text=[f\"avg profit {round(max_profit/num_profit, 2)} / {num_profit}\", \n",
    "                 f\"avg loss: {round(max_loss/num_loss, 2)} / {num_loss}\"], textposition='auto'),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=all_profit, showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "# Set theme, margin, and annotation in layout\n",
    "fig.update_layout(\n",
    "    template=\"plotly_dark\",\n",
    "    margin=dict(r=100, t=25, b=50, l=60),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=f\"H1 H4 atr1 kijun_sen_1 no neutral\",\n",
    "            showarrow=False,\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            x=0,\n",
    "            y=0)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T09:06:32.006143Z",
     "start_time": "2022-03-01T09:06:31.801144Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "macd, macdsignal, macdhist = talib.MACD(df.close)\n",
    "macdsignal = macdsignal.dropna()[-5:]\n",
    "\n",
    "x = np.array([x for x in range(len(macdsignal.index))])\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "y = scaler.fit_transform(macdsignal.values.reshape(-1, 1))\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y.flatten())\n",
    "print(\"slope: %f, intercept: %f\" % (slope, intercept))\n",
    "print(\"R-squared: %f\" % r_value**2)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(x, y, 'o', label='original data')\n",
    "plt.plot(x, intercept + slope*x, 'r', label='fitted line')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T17:03:36.211893Z",
     "start_time": "2022-02-19T17:03:32.068164Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import MetaTrader5 as mt5\n",
    "# display data on the MetaTrader 5 package\n",
    "print(\"MetaTrader5 package author: \",mt5.__author__)\n",
    "print(\"MetaTrader5 package version: \",mt5.__version__)\n",
    "\n",
    "# import the 'pandas' module for displaying data obtained in the tabular form\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500) # number of columns to be displayed\n",
    "pd.set_option('display.width', 1500)      # max table width to display\n",
    "# import pytz module for working with time zone\n",
    "import pytz\n",
    " \n",
    "# establish connection to MetaTrader 5 terminal\n",
    "if not mt5.initialize():\n",
    "    print(\"initialize() failed, error code =\",mt5.last_error())\n",
    "    quit()\n",
    "\n",
    "# set time zone to UTC\n",
    "timezone = pytz.timezone(\"Etc/UTC\")\n",
    "# create 'datetime' objects in UTC time zone to avoid the implementation of a local time zone offset\n",
    "utc_to = datetime.utcnow() + timedelta(days = 0, hours = 9, minutes=0)\n",
    "utc_from = utc_to - timedelta(days = 0, hours = 10, minutes=0)\n",
    "# request AUDUSD ticks within 11.01.2020 - 11.01.2020\n",
    "ticks = mt5.copy_ticks_range(\"XAUUSD\", utc_from, utc_to, mt5.COPY_TICKS_ALL)\n",
    "print(\"Ticks received:\",len(ticks))\n",
    " \n",
    "# shut down connection to the MetaTrader 5 terminal\n",
    "# mt5.shutdown()\n",
    "#  \n",
    "# display data on each tick on a new line\n",
    "print(\"Display obtained ticks 'as is'\")\n",
    "count = 0\n",
    "for tick in ticks:\n",
    "    count+=1\n",
    "    print(tick)\n",
    "    if count >= 10:\n",
    "        break\n",
    "\n",
    "# create DataFrame out of the obtained data\n",
    "ticks_frame = pd.DataFrame(ticks)\n",
    "# convert time in seconds into the datetime format\n",
    "ticks_frame['time']=pd.to_datetime(ticks_frame['time'], unit='s')\n",
    "ticks_frame['price'] = (ticks_frame['bid'] + ticks_frame['ask']) / 2\n",
    "# display data\n",
    "print(\"\\nDisplay dataframe with ticks\")\n",
    "print(ticks_frame.tail(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T15:11:07.278118Z",
     "start_time": "2022-02-11T15:11:07.123116Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# request AUDUSD ticks within 11.01.2020 - 11.01.2020\n",
    "ticks = mt5.copy_ticks_range(\"XAUUSD\", utc_from, utc_to, mt5.COPY_TICKS_ALL)\n",
    "print(\"Ticks received:\",len(ticks))\n",
    "# create DataFrame out of the obtained data\n",
    "ticks_frame = pd.DataFrame(ticks)\n",
    "# convert time in seconds into the datetime format\n",
    "ticks_frame['time']=pd.to_datetime(ticks_frame['time'], unit='s')\n",
    "ticks_frame['price'] = (ticks_frame['bid'] + ticks_frame['ask']) / 2\n",
    "ticks_frame = ticks_frame[['time','bid','ask','price']]\n",
    "# print(ticks_frame.tail())\n",
    "ticks_frame.set_index('time').loc[:,'bid':'ask']\n",
    "datetime_index = pd.DatetimeIndex(ticks_frame.time)\n",
    "df2=ticks_frame.set_index(datetime_index)\n",
    "OHLCData = df2.resample('5min').agg({'bid':'max','ask':'max','price':'ohlc'})\n",
    "OHLCData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T15:11:08.426465Z",
     "start_time": "2022-02-11T15:11:07.644117Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_ta as ta\n",
    "\n",
    "rates_frame = mt5.copy_rates_from_pos('BTCUSD', mt5.TIMEFRAME_M30, 0, 100)\n",
    "#     print(rates_frame)\n",
    "# create DataFrame out of the obtained data\n",
    "df = pd.DataFrame(rates_frame, columns=['time', 'open', 'high', 'low', 'close'])\n",
    "df['Date'] = pd.to_datetime(df['time'], unit='s')\n",
    "df = df.drop(['time'], axis=1)\n",
    "df = df.reindex(columns=['Date', 'open', 'high', 'low', 'close'])\n",
    "\n",
    "# convert to float to avoid sai so.\n",
    "df.open = df.open.astype(float)\n",
    "df.high = df.high.astype(float)\n",
    "df.low = df.low.astype(float)\n",
    "ticker = df.close.astype(float)\n",
    "\n",
    "# Get the 26-day EMA of the closing price\n",
    "k = df['close'].ewm(span=12, adjust=False, min_periods=12).mean()\n",
    "# Get the 12-day EMA of the closing price\n",
    "d = df['close'].ewm(span=26, adjust=False, min_periods=26).mean()\n",
    "# Subtract the 26-day EMA from the 12-Day EMA to get the MACD\n",
    "macd = k - d\n",
    "# Get the 9-Day EMA of the MACD for the Trigger line\n",
    "macd_s = macd.ewm(span=9, adjust=False, min_periods=9).mean()\n",
    "# Calculate the difference between the MACD - Trigger for the Convergence/Divergence value\n",
    "macd_h = macd - macd_s\n",
    "# Add all of our new values for the MACD to the dataframe\n",
    "# df['macd'] = df.index.map(macd)\n",
    "# df['macd_h'] = df.index.map(macd_h)\n",
    "# df['macd_s'] = df.index.map(macd_s)\n",
    "\n",
    "macd.plot(label='AAPL MACD', color='g')\n",
    "ax = macd_s.plot(label='Signal Line', color='r')\n",
    "ticker.plot(ax=ax, secondary_y=True, label='AAPL')\n",
    "\n",
    "ax.set_ylabel('MACD')\n",
    "ax.right_ax.set_ylabel('Price $')\n",
    "ax.set_xlabel('Date')\n",
    "lines = ax.get_lines() + ax.right_ax.get_lines()\n",
    "ax.legend(lines, [l.get_label() for l in lines], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T16:12:28.599366Z",
     "start_time": "2022-04-04T16:12:28.385350Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from livelossplot import PlotLossesKeras\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T09:31:33.910580Z",
     "start_time": "2022-03-01T09:31:33.882570Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T10:10:00.030127Z",
     "start_time": "2022-03-01T10:10:00.020122Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_tmp = []\n",
    "y_train_tmp = []\n",
    "for x_tmp, y_tmp in zip(X, y):\n",
    "#     if y_tmp == 1:\n",
    "    X_train_tmp.append(np.array(x_tmp).flatten())\n",
    "    y_train_tmp.append(y_tmp)\n",
    "    \n",
    "X_train = np.array(X_train_tmp)\n",
    "y_train = np.array(y_train_tmp)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_new)\n",
    "# y_train = y_new\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# n_in = len(X_train)\n",
    "# sequence = X_train.reshape((n_in, 100, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T10:10:01.444621Z",
     "start_time": "2022-03-01T10:10:01.433605Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T10:10:02.895399Z",
     "start_time": "2022-03-01T10:10:02.836149Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = pd.DataFrame(y_train, columns=[\"price\"])\n",
    "fig = px.histogram(df, x=\"price\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T09:38:10.657475Z",
     "start_time": "2022-03-01T09:38:10.481477Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:39:46.107034Z",
     "start_time": "2022-02-09T16:39:46.078035Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "scaler_filename = \"scaler.save\"\n",
    "joblib.dump(scaler, scaler_filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T09:32:23.502823Z",
     "start_time": "2022-03-01T09:32:23.496824Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "# from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T08:56:30.646896Z",
     "start_time": "2022-02-06T08:56:16.323073Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='relu', input_shape=(100,6)))\n",
    "model.add(RepeatVector(100))\n",
    "model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(6)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "# ADAM = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False, clipnorm=1.)\n",
    "# model.compile(optimizer=ADAM, loss='mse')\n",
    "model.fit(sequence, sequence, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T08:57:06.984957Z",
     "start_time": "2022-02-06T08:57:05.896960Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot_model(model, show_shapes=True, to_file='reconstruct_lstm_autoencoder.png')\n",
    "# demonstrate recreation\n",
    "yhat = model.predict(sequence, verbose=0)\n",
    "print(yhat[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T08:57:26.432949Z",
     "start_time": "2022-02-06T08:57:26.417944Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sequence[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T08:58:25.661218Z",
     "start_time": "2022-02-06T08:58:25.618203Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=8, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(\"relu\")(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=8, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation(\"relu\")(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=8, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation(\"relu\")(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(1, activation=\"sigmoid\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T07:45:45.446746Z",
     "start_time": "2022-02-06T07:45:12.538101Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 16\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "#     keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "    PlotLossesKeras()\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"best_model.h5\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T09:33:04.773968Z",
     "start_time": "2022-03-01T09:33:04.044052Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T10:10:17.301449Z",
     "start_time": "2022-03-01T10:10:16.195679Z"
    }
   },
   "outputs": [],
   "source": [
    "distortions = []\n",
    "inertias = []\n",
    "mapping1 = {}\n",
    "mapping2 = {}\n",
    "K = range(1, 10)\n",
    " \n",
    "for k in K:\n",
    "    # Building and fitting the model\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(X_train)\n",
    " \n",
    "    distortions.append(sum(np.min(cdist(X_train, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) /X_train.shape[0])\n",
    "    inertias.append(kmeanModel.inertia_)\n",
    " \n",
    "    mapping1[k] = sum(np.min(cdist(X_train, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X_train.shape[0]\n",
    "    mapping2[k] = kmeanModel.inertia_\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method using Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T10:10:23.909365Z",
     "start_time": "2022-03-01T10:10:23.737367Z"
    }
   },
   "outputs": [],
   "source": [
    "x_plot = X_train[:, 0]\n",
    "y_plot = X_train[:, 1]\n",
    "plt.scatter(x_plot, y_plot, s=y_train, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T16:58:48.372363Z",
     "start_time": "2022-02-15T16:58:48.362364Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T10:10:29.978258Z",
     "start_time": "2022-03-01T10:10:29.381258Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "x_train_pca = pca.fit_transform(X_train)\n",
    "# x_test_pca = pca.transform(X_test)\n",
    "\n",
    "x_plot = x_train_pca[:, 0]\n",
    "y_plot = x_train_pca[:, 1]\n",
    "\n",
    "t0 = time.time()\n",
    "n_clusters = 3\n",
    "k_means_model = KMeans(n_clusters=n_clusters, random_state=0, max_iter=1000).fit(X_train)\n",
    "sprectal_clustering_model = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans')\n",
    "# logistic = LogisticRegression(random_state=0, max_iter=10000).fit(X_train, y_train)\n",
    "# liner_model = LinearRegression().fit(X_train, y_train)\n",
    "t_batch = time.time() - t0\n",
    "\n",
    "k_means_model.labels_\n",
    "\n",
    "def set_color(value):\n",
    "    return \"r\" if value < 0 else \"g\"\n",
    "\n",
    "def set_sizes(value):\n",
    "    return abs(value) * 10\n",
    "\n",
    "y_pred = k_means_model.predict(X_train)\n",
    "# yhat_probs = logistic.predict_proba(X_train)[:, 1]\n",
    "# reduce to 1d array\n",
    "# convert = lambda x: 1 if x > 0.9 else 0\n",
    "# y_pred = list(map(convert, yhat_probs ))\n",
    "\n",
    "sizes = list(map(set_sizes, y_train))\n",
    "colors = list(map(set_color, y_train))\n",
    "plt.scatter(x_plot, y_plot, c=colors, s=sizes, cmap='viridis')\n",
    "\n",
    "# centers = k_means_model.cluster_centers_\n",
    "# plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5);\n",
    "\n",
    "# colors = list(map(set_color, y_pred))\n",
    "\n",
    "# x_plot = x_train_pca[:, 0]\n",
    "# y_plot = x_train_pca[:, 1]\n",
    "# plt.scatter(x_plot, y_plot, c=colors,s=sizes, alpha=1)\n",
    "plt.colorbar();  # show color scale\n",
    "\n",
    "# clusters_profit = [0]*n_clusters\n",
    "for i, (label_train, predict_cluster) in enumerate(zip(y_train, y_pred)):\n",
    "#     center_point = centers[predict_cluster]\n",
    "#     distance = np.linalg.norm(center_point-(x_plot[i], y_plot[i]))\n",
    "#     if distance < 3:\n",
    "    plt.annotate(f\"{round(label_train, 1)}\", (x_plot[i], y_plot[i]))\n",
    "#     plt.scatter(x_plot[i], y_plot[i], c=predict_cluster, s=label_train, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T09:33:30.687748Z",
     "start_time": "2022-03-01T09:33:30.577917Z"
    }
   },
   "outputs": [],
   "source": [
    "px.histogram(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:30:17.658739Z",
     "start_time": "2022-02-09T16:30:17.650730Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:26:52.603297Z",
     "start_time": "2022-02-09T16:26:52.592292Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-06T07:49:38.153824Z",
     "start_time": "2022-02-06T07:49:37.938822Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = k_means_model.predict(X_test)\n",
    "colors = list(map(set_color, y_pred))\n",
    "sizes = list(map(set_sizes, y_pred))\n",
    "x_plot = x_test_pca[:, 0]\n",
    "y_plot = x_test_pca[:, 1]\n",
    "plt.scatter(x_plot, y_plot, c=colors, s=sizes)\n",
    "plt.title(\"Incorrect Number of Blobs\")\n",
    "for i, txt in enumerate(y_test):\n",
    "    plt.annotate(round(txt, 1), (x_plot[i], y_plot[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"best_model.h5\")\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# print(\"Test accuracy\", test_acc)\n",
    "# print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:32:01.555969Z",
     "start_time": "2022-02-09T16:32:01.549957Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T04:49:11.008220Z",
     "start_time": "2022-02-10T04:49:10.989220Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#Confution Matrix and Classification Report\n",
    "#Y_pred = model.predict(X_test)\n",
    "#y_pred = np.argmax(Y_pred, axis=0)\n",
    "\n",
    "# predict probabilities for test se\n",
    "y_pred = logistic.predict(X_train)\n",
    "# reduce to 1d array\n",
    "# convert = lambda x: 1 if x > 0.5 else 0\n",
    "# y_pred = list(map(convert, y_pred ))\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['0', '1']\n",
    "print(classification_report(y_train, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T04:58:49.376256Z",
     "start_time": "2022-02-10T04:58:49.357254Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import roc_curve\n",
    "# ROC AUC\n",
    "yhat_probs = logistic.predict_proba(X_train)[:, 1]\n",
    "auc = roc_auc_score(y_train, yhat_probs)\n",
    "print('ROC AUC: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T05:09:51.660034Z",
     "start_time": "2022-02-10T05:09:51.487038Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "lr_probs = logistic.predict_proba(X_train)[:, 1]\n",
    "# keep probabilities for the positive outcome only\n",
    "ns_probs = [0 for _ in range(len(X_train))]\n",
    "ns_auc = roc_auc_score(y_train, ns_probs)\n",
    "lr_auc = roc_auc_score(y_train, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, ns_thresholds  = roc_curve(y_train, ns_probs)\n",
    "lr_fpr, lr_tpr, lr_thresholds  = roc_curve(y_train, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, linestyle='--', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T05:09:59.955875Z",
     "start_time": "2022-02-10T05:09:59.943751Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T05:10:01.287396Z",
     "start_time": "2022-02-10T05:10:01.278398Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T05:10:02.714942Z",
     "start_time": "2022-02-10T05:10:02.695943Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tick_data(utc_from, utc_to, timeframe, symbol):\n",
    "    mapping_timeframe = {\n",
    "        mt5.TIMEFRAME_H1: \"60min\",\n",
    "        mt5.TIMEFRAME_M15: \"15min\",\n",
    "        mt5.TIMEFRAME_M5: \"5min\",\n",
    "        mt5.TIMEFRAME_M30: \"30min\",\n",
    "        mt5.TIMEFRAME_D1: \"D1\",\n",
    "        mt5.TIMEFRAME_H4: \"H4\",\n",
    "    }\n",
    "    map_timeframe = mapping_timeframe.get(timeframe)\n",
    "    # request AUDUSD ticks within 11.01.2020 - 11.01.2020\n",
    "    ticks = mt5.copy_ticks_range(symbol, utc_from, utc_to, mt5.COPY_TICKS_ALL)\n",
    "#     print(\"Ticks received:\",len(ticks))\n",
    "    # create DataFrame out of the obtained data\n",
    "    ticks_frame = pd.DataFrame(ticks)\n",
    "    # convert time in seconds into the datetime format\n",
    "    ticks_frame['time']=pd.to_datetime(ticks_frame['time'], unit='s')\n",
    "    ticks_frame['price'] = (ticks_frame['bid'] + ticks_frame['ask']) / 2\n",
    "    ticks_frame = ticks_frame[['time','bid','ask','price']]\n",
    "    # print(ticks_frame.tail())\n",
    "    ticks_frame.set_index('time').loc[:,'bid':'ask']\n",
    "    datetime_index = pd.DatetimeIndex(ticks_frame.time)\n",
    "    ticks_frame=ticks_frame.set_index(datetime_index)\n",
    "    OHLCData = ticks_frame.resample(map_timeframe).agg({'bid':'max','ask':'max','price':'ohlc'})\n",
    "    df = pd.DataFrame(OHLCData.price, columns=['open', 'high', 'low', 'close'])\n",
    "    df['bid'] = OHLCData.bid['bid']\n",
    "    df['ask'] = OHLCData.ask['ask']\n",
    "    df['Date'] = OHLCData.index\n",
    "    df = df.reindex(columns=['Date', 'open', 'high', 'low', 'close', 'bid', 'ask'])\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(['time'], axis=1)\n",
    "    #df['EMA_50'] = df.close.ewm(span=50, adjust=False).mean()\n",
    "    df['EMA12'] = df.close.ewm(span=12, adjust=False).mean()\n",
    "    df['EMA26'] = df.close.ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = df['EMA26'] - df['EMA12']\n",
    "    df['MACDSignalLine'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    df['HIST'] = df['MACD'] - df['MACDSignalLine']\n",
    "    atr_multiplier = 3.0\n",
    "    atr_period = 10\n",
    "    supertrend = Supertrend(df, atr_period, atr_multiplier)\n",
    "    df = df.join(supertrend)\n",
    "    lookback = 14\n",
    "    conditions = [\n",
    "        (df['Supertrend10'] == True) & (df['HIST'] > 0) & (df['HIST'] > df['HIST'].shift(2)),\n",
    "        (df['Supertrend10'] == False) & (df['HIST'] < 0) & (df['HIST'] < df['HIST'].shift(2))\n",
    "    ]\n",
    "    values = ['Buy', 'Sell']\n",
    "    df['Trend'] = np.select(conditions, values)\n",
    "#     print(df.tail(10))\n",
    "    return df.Date.iat[-1], df.Trend.iat[-1], df.bid.iat[-1], df.ask.iat[-1], \n",
    "\n",
    "def Supertrend(df, atr_period, multiplier):\n",
    "    \n",
    "    high = df['high']\n",
    "    low = df['low']\n",
    "    close = df['close']\n",
    "    \n",
    "    # calculate ATR\n",
    "    price_diffs = [high - low, \n",
    "                   high - close.shift(), \n",
    "                   close.shift() - low]\n",
    "    true_range = pd.concat(price_diffs, axis=1)\n",
    "    true_range = true_range.abs().max(axis=1)\n",
    "    # default ATR calculation in supertrend indicator\n",
    "    atr = true_range.ewm(alpha=1/atr_period,min_periods=atr_period).mean() \n",
    "    # df['atr'] = df['tr'].rolling(atr_period).mean()\n",
    "    \n",
    "    # HL2 is simply the average of high and low prices\n",
    "    hl2 = (high + low) / 2\n",
    "    # upperband and lowerband calculation\n",
    "    # notice that final bands are set to be equal to the respective bands\n",
    "    final_upperband = upperband = hl2 + (multiplier * atr)\n",
    "    final_lowerband = lowerband = hl2 - (multiplier * atr)\n",
    "    \n",
    "    # initialize Supertrend column to True\n",
    "    supertrend = [True] * len(df)\n",
    "    \n",
    "    for i in range(1, len(df.index)):\n",
    "        curr, prev = i, i-1\n",
    "        \n",
    "        # if current close price crosses above upperband\n",
    "        if close[curr] > final_upperband[prev]:\n",
    "            supertrend[curr] = True\n",
    "        # if current close price crosses below lowerband\n",
    "        elif close[curr] < final_lowerband[prev]:\n",
    "            supertrend[curr] = False\n",
    "        # else, the trend continues\n",
    "        else:\n",
    "            supertrend[curr] = supertrend[prev]\n",
    "            \n",
    "            # adjustment to the final bands\n",
    "            if supertrend[curr] == True and final_lowerband[curr] < final_lowerband[prev]:\n",
    "                final_lowerband[curr] = final_lowerband[prev]\n",
    "            if supertrend[curr] == False and final_upperband[curr] > final_upperband[prev]:\n",
    "                final_upperband[curr] = final_upperband[prev]\n",
    "\n",
    "        # to remove bands according to the trend direction\n",
    "        if supertrend[curr] == True:\n",
    "            final_upperband[curr] = np.nan\n",
    "        else:\n",
    "            final_lowerband[curr] = np.nan\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        f'Supertrend{atr_period}': supertrend,\n",
    "        f'FinalLowerband{atr_period}': final_lowerband,\n",
    "        f'FinalUpperband{atr_period}': final_upperband\n",
    "    }, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T13:19:45.394315Z",
     "start_time": "2022-04-05T13:19:45.385316Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks, find_peaks_cwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T14:04:19.962764Z",
     "start_time": "2022-04-11T14:04:19.786759Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "current_time = datetime.now().hour % 4\n",
    "rates_frame = mt5.copy_rates_from_pos(\"XAUUSD\", mt5.TIMEFRAME_H4, 0, 100)\n",
    "df = pd.DataFrame(rates_frame, columns=['time', 'open', 'high', 'low', 'close'])\n",
    "df['Date'] = pd.to_datetime(df['time'], unit='s')\n",
    "df = df.drop(['time'], axis=1)\n",
    "df = df.reindex(columns=['Date', 'open', 'high', 'low', 'close'])\n",
    "df['macd'], df['signal'], df['histogram'] = talib.MACD(df.close)\n",
    "df['rsi'] = talib.RSI(df.close)\n",
    "df['atr'] = talib.ATR(df.high, df.low, df.close)\n",
    "df = df.dropna()\n",
    "df.tail()\n",
    "\n",
    "\n",
    "last_point = len(df.signal.values)\n",
    "x = df.signal.values\n",
    "close = df.close.values\n",
    "macd = df.macd.values\n",
    "scaler = MinMaxScaler(feature_range=(np.min(x), np.max(x)))\n",
    "close = scaler.fit_transform(close.reshape(-1, 1)).flatten()\n",
    "\n",
    "x1 = np.array([tmp for tmp in x])\n",
    "maxima = argrelextrema(x1, np.greater)\n",
    "\n",
    "x2 = np.array([tmp for tmp in x])\n",
    "minima = argrelextrema(x2, np.less)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot([x for x in range(len(x))], x)\n",
    "\n",
    "plt.scatter(maxima, [x[min_val] for min_val in maxima])\n",
    "plt.scatter(minima, [x[min_val] for min_val in minima])\n",
    "plt.plot([x for x in range(len(close))], x)\n",
    "plt.plot([x for x in range(len(close))], macd)\n",
    "plt.show()\n",
    "\n",
    "np.intersect1d(df['macd'].values, df['signal'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T04:20:22.849385Z",
     "start_time": "2022-04-11T04:20:22.835384Z"
    }
   },
   "outputs": [],
   "source": [
    "maxima[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:46:26.222372Z",
     "start_time": "2022-03-25T14:46:26.208372Z"
    }
   },
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T13:20:06.183180Z",
     "start_time": "2022-04-05T13:19:59.722185Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T13:20:06.292164Z",
     "start_time": "2022-04-05T13:20:06.278164Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = './checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T13:20:06.402734Z",
     "start_time": "2022-04-05T13:20:06.388168Z"
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(len(df) * 0.9)\n",
    "test_size = len(df) - train_size\n",
    "train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n",
    "test = test.reset_index(drop=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T13:20:07.706225Z",
     "start_time": "2022-04-05T13:20:07.674225Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import dump, load\n",
    "features = ['histogram', 'macd', 'signal']\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = load('scaler.save') \n",
    "train_scaler = scaler.fit_transform(train[features])\n",
    "test_scaler = scaler.transform(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T13:20:13.224753Z",
     "start_time": "2022-04-05T13:20:13.216760Z"
    }
   },
   "outputs": [],
   "source": [
    "train_scaler.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T13:20:25.357848Z",
     "start_time": "2022-04-05T13:20:25.337841Z"
    }
   },
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(raw_frame, sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix >= len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], end_ix\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T13:20:26.792658Z",
     "start_time": "2022-04-05T13:20:26.754656Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(X, time_steps=1):\n",
    "    X1, y1 = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        t = X[i:(i + time_steps)]\n",
    "        X1.append(t)\n",
    "    return np.array(X1)\n",
    "TIME_STEPS = 50\n",
    "X_train, y_train = split_sequence(\n",
    "    train,\n",
    "    train_scaler,\n",
    "    TIME_STEPS\n",
    ")\n",
    "X_test, y_test = split_sequence(\n",
    "    test,\n",
    "    test_scaler,\n",
    "    TIME_STEPS\n",
    ")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T13:20:32.002436Z",
     "start_time": "2022-04-05T13:20:31.995428Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train = np.array(train_X)\n",
    "# train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T13:28:12.613720Z",
     "start_time": "2022-04-05T13:28:11.441727Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timesteps, input_dim = X_train.shape[1], X_train.shape[2]\n",
    "latent_dim = 64\n",
    "inputs = tf.keras.Input(shape=(timesteps, input_dim))\n",
    "encoded = tf.keras.layers.LSTM(64, return_sequences=True)(inputs)\n",
    "encoded = tf.keras.layers.Dropout(rate=0.2)(encoded)\n",
    "encoded = tf.keras.layers.LSTM(latent_dim, return_sequences=True)(encoded)\n",
    "encoded = tf.keras.layers.LSTM(latent_dim)(encoded)\n",
    "decoded = tf.keras.layers.RepeatVector(timesteps)(encoded)\n",
    "decoded = tf.keras.layers.LSTM(latent_dim, return_sequences=True)(decoded)\n",
    "# decoded = tf.keras.layers.LSTM(64, return_sequences=True)(encoded)\n",
    "decoded = tf.keras.layers.Dropout(rate=0.2)(decoded)\n",
    "decoded = tf.keras.layers.LSTM(input_dim, return_sequences=True)(decoded)\n",
    "\n",
    "sequence_autoencoder = tf.keras.Model(inputs, decoded)\n",
    "encoder = tf.keras.Model(inputs, encoded)\n",
    "sequence_autoencoder.compile(loss='mse', optimizer='adam')\n",
    "# sequence_autoencoder.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T14:29:31.289509Z",
     "start_time": "2022-04-05T13:28:29.514034Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "history = sequence_autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=2000,\n",
    "    batch_size=32,\n",
    "    validation_split=0.3,\n",
    "    shuffle=False,\n",
    "    callbacks=[callback, model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T14:29:47.775476Z",
     "start_time": "2022-04-05T14:29:47.651478Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence_autoencoder.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T14:30:05.023757Z",
     "start_time": "2022-04-05T14:30:05.012741Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# y_pred = sequence_autoencoder.predict(X_train, batch_size=64, verbose=1)\n",
    "# y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# print(classification_report(train_y, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T14:30:26.752883Z",
     "start_time": "2022-04-05T14:30:26.597882Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T07:44:27.330261Z",
     "start_time": "2022-03-19T07:44:21.647321Z"
    }
   },
   "outputs": [],
   "source": [
    "# pred = sequence_autoencoder.predict(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T14:30:37.475390Z",
     "start_time": "2022-04-05T14:30:34.677846Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding = encoder.predict(X_train, verbose=0)\n",
    "embedding_test = encoder.predict(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T07:57:08.062361Z",
     "start_time": "2022-03-19T07:57:08.049361Z"
    }
   },
   "outputs": [],
   "source": [
    "# tmp_scaler = MinMaxScaler()\n",
    "# embedding = tmp_scaler.fit_transform(embedding)\n",
    "# embedding_test = tmp_scaler.transform(embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T13:48:02.758864Z",
     "start_time": "2022-03-19T13:48:02.746867Z"
    }
   },
   "outputs": [],
   "source": [
    "# from joblib import dump, load\n",
    "# dump(scaler, 'scaler.save') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T14:30:40.795819Z",
     "start_time": "2022-04-05T14:30:40.748815Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from mpl_finance import candlestick_ohlc\n",
    "import matplotlib.dates as mpl_dates\n",
    "import matplotlib.dates as md\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T14:30:42.665516Z",
     "start_time": "2022-04-05T14:30:42.651517Z"
    }
   },
   "outputs": [],
   "source": [
    "width = .4\n",
    "width2 = .05\n",
    "#define colors to use\n",
    "col1 = 'green'\n",
    "col2 = 'red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T13:46:39.074422Z",
     "start_time": "2022-03-19T13:46:38.977422Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save('embedding.npy', embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T14:44:51.479225Z",
     "start_time": "2022-04-05T14:43:17.351567Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, subfig = plt.subplots(4, 1)\n",
    "plot_price = False\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "for plot_point in range(len(y_test) - 1):\n",
    "    sample = embedding_test[plot_point]\n",
    "    min_dist = 100\n",
    "    curr = []\n",
    "    all_dist = []\n",
    "\n",
    "    similar_raw = cosine_similarity([sample], embedding)[0][1:]\n",
    "    top_maximize = similar_raw.argsort()[-4:][::-1]\n",
    "    similar_values = []\n",
    "    for item in top_maximize:\n",
    "        similar_values.append(similar_raw[item])\n",
    "    print(similar_values, top_maximize)\n",
    "    \n",
    "    ax1 = subfig[0]\n",
    "    ax1.clear()\n",
    "    ax1.title.set_text(f'{test.Date.values[y_test[plot_point]]}%')\n",
    "    fig.suptitle('Horizontally stacked subplots')\n",
    "    ohlc = test.iloc[(plot_point):(plot_point+TIME_STEPS+TIME_STEPS)]\n",
    "    up = ohlc[ohlc.close>=ohlc.open]\n",
    "    down = ohlc[ohlc.close<ohlc.open]\n",
    "\n",
    "    if plot_price:\n",
    "        ax1.bar(up.index,up.close-up.open,width,bottom=up.open,color=col1)\n",
    "        ax1.bar(up.index,up.high-up.close,width2,bottom=up.close,color=col1)\n",
    "        ax1.bar(up.index,up.low-up.open,width2,bottom=up.open,color=col1)\n",
    "        ax1.bar(down.index,down.close-down.open,width,bottom=down.open,color=col2)\n",
    "        ax1.bar(down.index,down.high-down.open,width2,bottom=down.open,color=col2)\n",
    "        ax1.bar(down.index,down.low-down.close,width2,bottom=down.close,color=col2)\n",
    "        ax1.scatter([test.index[y_test[plot_point]]], [test.close.values[y_test[plot_point]]])\n",
    "    else:\n",
    "        ax1.plot(ohlc.index, ohlc.macd.values)\n",
    "        ax1.plot(ohlc.index, ohlc.histogram.values)\n",
    "        ax1.plot(ohlc.index, ohlc.signal.values)\n",
    "        ax1.scatter([test.index[y_test[plot_point]]], [test.macd.values[y_test[plot_point]]])\n",
    "\n",
    "    new_ploted = False\n",
    "    for ax, plot_point, similar_value in zip(subfig[1:], top_maximize, similar_values):\n",
    "        ax.clear()\n",
    "        ax.title.set_text(f'{train.Date.values[y_train[plot_point]]} Similar {similar_value * 100}%')\n",
    "\n",
    "        ohlc = train.iloc[(plot_point):(plot_point+TIME_STEPS+TIME_STEPS)]\n",
    "        up = ohlc[ohlc.close>=ohlc.open]\n",
    "        down = ohlc[ohlc.close<ohlc.open]\n",
    "        if plot_price:\n",
    "            ax.bar(up.index,up.close-up.open,width,bottom=up.open,color=col1)\n",
    "            ax.bar(up.index,up.high-up.close,width2,bottom=up.close,color=col1)\n",
    "            ax.bar(up.index,up.low-up.open,width2,bottom=up.open,color=col1)\n",
    "            ax.bar(down.index,down.close-down.open,width,bottom=down.open,color=col2)\n",
    "            ax.bar(down.index,down.high-down.open,width2,bottom=down.open,color=col2)\n",
    "            ax.bar(down.index,down.low-down.close,width2,bottom=down.close,color=col2)\n",
    "            ax.scatter([train.index[y_train[plot_point]]], [train.close.values[y_train[plot_point]]])\n",
    "        else:\n",
    "            ax.plot(ohlc.index, ohlc.macd.values)\n",
    "            ax.plot(ohlc.index, ohlc.histogram.values)\n",
    "            ax.plot(ohlc.index, ohlc.signal.values)\n",
    "            ax.scatter([train.index[y_train[plot_point]]], [train.macd.values[y_train[plot_point]]])\n",
    "        new_ploted = True\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
